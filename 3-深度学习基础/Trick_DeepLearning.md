# 深度学习中的一些Trick 

---

# 1,Trick - 融合训练集，验证集，测试集

。。。。。

# 2,Trick - 提前终止

tags: 深度学习

------

前期对该Trick的重视度不够， 想着，不就是过拟合吗？ 我只要保存 验证集 loss 最低点的模型， 你过拟合管我什么事情， 然而， 上了 Bert + 中大规模数据集之后， 我才发现什么叫做噩梦 -- 训练时间太 bb 长了吧。 

那还能咋地， 你总不能把 epoch 数量设置的太小吧， 10 是比较基本的吧（看数据集)，但这样跑也能跑个一天一夜，枯了。 这个时候这个Trick 就很有用了，大大节省模型训练时间。

提前终止指的是**当验证集的误差连续 `n`次递增时停止训练**。 一般情况下， 我会先将`n`设置为一个较大的数， 跑一个较大的 epoch（如20）， 然后根据 loss 曲线来选择合适的 epoch 数量和 `n`， 接下来就欢快的嘿嘿嘿了。

需要注意的是，提前终止有其他很多策略，上面提到的只是很常用的一种，至于什么时候选择什么样的提前终止策略也是一个经验活， 还好， [1] 中为我们提供了一些参考， 值得一看。 由于本人经验时间优先，没有深入探讨，希望有大佬帮忙解惑。下面我简单提一下[1] 中是如何建议的。

## Early Stop, but when? [1]

首先，思考一个问题： 如果我们以验证集loss最小值来作为保存模型的依据，那么这会不会对模型的泛化能力产生影响？我个人认为是**不会**，具体需要看Loss曲线图。一般而言，第一次跑的时候，我往往会设置一个很大的 epoch， 这样能够直观的从Loss曲线上看到很多信息，从而帮助我来选择更佳合适 epoch数量， 我个人一般取开始过拟合之后两个epoch作为epoch的最终数量。

我自己做实验发现，在验证集loss最低点不一定能够获得在测试集上最好的效果，有时候，最低点附近的一些点反而在测试集上表现更佳，但差别不大，这也是我认为不会影响模型泛化能力的一个依据， 而实际落地中，这点差别其实真的没有啥鸟用。

回到Trick， 文章谈到， 何时提前停止是对训练时间和泛化误差之间的权衡， 而如果我采用在验证集loss最低点存储模型的方式，那么何时提前停止就不再是对泛化误差的权衡了， 其变成了，如何选择何种的策略来对**训练时间**以及**获取验证集loss全局最低点**来做权衡，使得我们能够找到该点并保存模型，而不至于陷入一些困境[2]。

最后谈谈 Early Stop 策略， 选择合适的策略能够使得我们在合适的时间终止整个训练，从而节省时间。



# 3,Trick - 学习率衰减

tags: 深度学习

------

## 为什么要进行学习率衰减？

随着训练的加深，当模型训练到一定程度后，损失将不再减少，这个时候模型的一阶梯度接近于0， 对应的 Hessian 矩阵通常是两种情况：

- 正定，即所有特征值均为正，此时通常可以得到一个局部极小值，若这个局部极小值接近全局最小则模型已经能得到不错  的性能了，但若差距很大，则模型性能还有待于提升，通常情况下后者在训练初最常见。
- 特征值有正有负，此时模型很可能陷入了鞍点，若陷入鞍点，模型性能表现就很差。

以上两种情况在训练初期以及中期，此时若仍然以固定的学习率，会使模型陷入左右来回的震荡或者鞍点，无法继续优化。所以，学习率衰减或者增大能帮助模型有效的减少震荡或者逃离鞍点。

## 学习率衰减策略

参见 500 questions 第14 章



# 4,Trick-正则化

------

https://blog.csdn.net/jinping_shi/article/details/52433975

https://zhuanlan.zhihu.com/p/38709373

https://blog.csdn.net/heyongluoyao8/article/details/49429629

## 1. L1 正则化 - 稀疏正则化

1-范数: 表示向量元素的绝对值之和。
$$
||x|| =\sum_{i=1}^N |x_i|
$$

$$
正则化项： \Omega(\theta) = ||w||_1 =  \sum_i |w_i| \\
目标函数： \tilde{J}(w;X,y) = \alpha ||w||_1  + J(w;X,y)  \\
梯度： \nabla_w \tilde{J}(w;X,y) = \alpha sign(w) + \nabla_w J(w;X,y) \\
$$

不同于L2，L1 正则化使得权重值可能被减少到0。 因此，L1对于压缩模型很有用。

稀疏向量通常会有许多维度，如果再加上使用特征组合会导致包含更多的维度的。由于使用此类高维度特征向量，因此模型可能会非常庞大，并且需要大量的 RAM。

在高维度稀疏矢量中，最好尽可能使权重正好降至 `0`。正好为 0 的权重基本上会使相应特征从模型中移除。 将特征设为 0 可节省 RAM 空间，且可以减少模型中的噪点。

## 2. L2 正则化 -- 权重衰减

2-范数： 表示向量元素绝对值的平方和再开方。
$$
||x|| = \sqrt{\sum_{i=1}^N x_i^2}
$$

$$
正则化项： \Omega(\theta) = \frac{1}{2} ||w||_2^2  = \frac{1}{2}w^Tw \\
目标函数： \tilde{J}(w;X,y) = \frac{\alpha}{2}w^Tw  + J(w;X,y)  \\
梯度： \nabla_w \tilde{J}(w;X,y) = \alpha w + \nabla_w J(w;X,y) \\
梯度更新 ： w \leftarrow (1- \epsilon \alpha) w - \epsilon \nabla_w J(w;X,y)
$$

L2正则化又称**权重衰减**。因为其导致权重**趋向于0**（但不全是0）。

执行 L2 正则化对模型具有以下影响：

- 使权重值接近于 0（但并非正好为 0）
- 使权重的平均值接近于 0，且呈正态分布。



------

## QA

### 1. 为何只对权重进行正则惩罚，而不针对偏置

在神经网络中，参数包括每一层仿射变换的**权重w**和**偏置b**，我们通常只对权重做惩罚而不对偏置做正则惩罚。

精确拟合偏置所需的数据通常比拟合权重少得多。每个权重会指定两个变量如何相互作用。我们需要在各种条件下观察这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。另外，正则化偏置参数可能会导致明显的欠拟合。

### 2. 权重衰减的目的

限制模型的学习能力，通过限制参数 θ 的规模（主要是权重 w 的规模，偏置 b 不参与惩罚），使模型偏好于权值较小的目标函数，防止过拟合。

### 3. L1 与 L2 的异同

- 相同点：限制模型的学习能力，通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。

- 不同点：

  > - L1是模型各个参数的绝对值之和；L2为各个参数平方和的开方值。
  > - L1 正则化可以产生**稀疏权值矩阵**，即产生一个稀疏模型，可以用于特征选择；L2 会趋向于生成一个参数值很小的矩阵。
  > - L1 适用于特征之间有关联的情况； L2 适用于特征之间没有关联的情况

### 4.为什么 L1 正则化 可以产生稀疏值，而 L2 不会？

添加 L1 正则化，相当于在 L1范数的约束下求目标函数 J 的最小值，下图展示了二维的情况：

![L1](D:/算法岗面试学习之路/NLPer-Interview-master/img/正则化/L1.png)

图中 J 与 L 首次相交的点就是最优解。L1 在和每个坐标轴相交的地方都会有“角”出现（多维的情况下，这些角会更多），在角的位置就会产生稀疏的解。而 J 与这些“角”相交的机会远大于其他点，因此 L1 正则化会产生稀疏的权值。

对于 L2 正则化来说，其二维平面下的图形为：

![L2](D:/算法岗面试学习之路/NLPer-Interview-master/img/正则化/L2.png)

如上图所示， 相比于 L1 正则化， L2 不会产生 “角”， 因此 J 与 L2 相交的点具有稀疏性的概率就会变得非常小。

### 5. 为何 L1 和 L2 正则化可以防止过拟合？

L1 & L2 正则化会使模型偏好于更小的权值。

简单来说，更小的权值意味着更低的模型复杂度，也就是对训练数据的拟合刚刚好，不会过分拟合训练数据（比如异常点，噪声），以提高模型的泛化能力。

此外，添加正则化相当于为模型添加了某种限制，规定了参数的分布，从而降低了模型的复杂度。模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。 -- **奥卡姆剃刀原理**



# 5,Trick - Dropout

------

## 思想

- **思想：** 在每次训练过程中随机地忽略一些神经元。这些神经元被随机地“抛弃”了。也就是说它们在正向传播过程中对于下游神经元的贡献效果暂时消失了，反向传播时该神经元也不会有任何权重的更新。
- **注意：**每次迭代的过程中，我们删除的神经单元是随机的，本次删除的与上次删除神经元是不一样的

简单来说，Dropout 通过**参数共享**提供了一种廉价的 Bagging 集成近似—— Dropout 策略相当于集成了包括所有从基础网络除去部分单元后形成的子网络。



------

## QA

### 1. 为何 Dropout 能够解决过拟合？

- **取平均的作用：**  先回到正常的模型（没有dropout），我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。

  dropout 掉不同的隐藏神经元就类似在训练不同的网络（随机删掉一半隐藏神经元导致网络结构已经不同)，整个dropout过程就相当于 对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。

- **减少神经元之间复杂的共适应关系：** 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况）。 迫使网络去学习更加鲁棒的特征 （这些特征在其它的神经元的随机子集中也存在）。

  换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。（这个角度看 dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高）

### 2. Drpout 与 Bagging 有何不同？

- 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型**共享参数**，其中每个模型继承父神经网络参数的不同子集。
- 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。

### 3. Dropout 有什么缺陷？

dropout一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数J每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。

我通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。我觉得你也可以尝试其它方法，虽然我们并没有关于这些方法性能的数据统计，但你可以把它们与dropout方法一起使用。  

# 6,Trick-Normalization

tags: 深度学习

------

## 0 . 归一化

### 1. 归一化手段

- **Min-max 归一化：**当有新数据加入时， 可能导致max和min的变化， 需要重新定义。
  $$
   x^* = \frac{x -min } {max - min} 
  $$

- **Zero-mean 归一化：**均值为0，标准差为1的标准正态分布。 z-score标准化方法适用于属性A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。该种标准化方式要求原始数据的分布可以近似为高斯分布，否则效果会变得很糟糕。
  $$
  x^* = \frac{x- \mu }{\sigma}
  $$

### 2. Min-max 与 Zero-mean 区别

- 对于输出结果范围有要求， 使用Min-max normalization 
- 数据较为稳定， 不存在极端最大值，最小值， 用归一化
- 如果数据存在异常值或较多噪音， 使用标准化。

### 3. 为何归一化为何如此优秀？

归一化的本质就是**线性变换**。 线性变化的诸多良好性质，决定了为什么对数据进行改变后不会造成“失效”，还能提高数据的表现。

**归一化加快了梯度下降求最优解的速度。**

假定一个预测房价的例子，两个特征： 面积与房间数，那么则有： $y = \theta_1 x_1 + \theta_2x_2$， $x_1$ 表房间数，$x_2$ 表面积 ， 现实中，面积范围往往在 $1-1000$， 而房间通常为 $1-10$， 那么面积对模型的影响要更大一些，此时寻求最优解的过程为：

![1](D:/算法岗面试学习之路/NLPer-Interview-master/img/Normalization/1.jpg)

归一化后，寻求最优解的过程为：

![2](D:/算法岗面试学习之路/NLPer-Interview-master/img/Normalization/2.jpg)

在未归一化的时候， 由于 $\theta_1$ 的更新幅度要比  $\theta_2$ 小， 因此  $\theta_2$ 的应该要比  $\theta_1$ 要大，但是在实际中，我们使用常规梯度下降法时，我们各个的学习率都是一样的，这也就造成了 $\theta_2$ 的更新会比较慢，结果就是寻求最优解的过程会走很多弯路导致模型收敛速度缓慢。

我们来实际举例， 假设在未归一化的时候， 我们的损失函数为：
$$
J = (3 \theta_1 + 600 \theta_2 - \hat{y})^2
$$
那么经过归一化后，我们的损失函数可能就变为：
$$
J = (0.5 \theta_1 + 0.55 \theta_2 - \hat{y})^2 
$$
很明显可以看到，数据归一化后， 最优解的寻找过程会很平缓，更容易正确收敛到最优解。

其次，还有一些博客中提到，**归一化有可能提高精度**， 这在涉及到一些距离计算的算法时效果显著。

## 1. Batch Normalization

假设一个 batch 为 m 个输入 $B = \{x_{1, \cdots, m}\}$ , BN 在这 m 个数据之间做 Normalization， 并学习参数 $\gamma , \beta$：
$$
\mu_B \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_i    \\
\sigma_B^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2  \\
\hat{x}_i  \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \\
y_i \leftarrow \gamma \hat{x}_i + \beta \equiv BN_{\gamma, \beta}{(x_i)}
$$
所以在使用BN后，Batch Size不能设置的太小。

## 2. Layer Normalization

LN 不同于 BN， 其在层内进行 Normalization， 即直接对隐层单元的输出做 Normalization。最大的好处是不再依赖 batch size。 
$$
u^l = \frac{1}{H} \sum_{i=1}^H a_i^l \\
\sigma^l = \sqrt{\frac{1}{H}\sum_{i=1}^H(a_i^l - u^l)^2} \\
\hat{a}_i^l  \leftarrow \frac{a_i^l - \mu^l}{\sqrt{\sigma_l^2 + \epsilon}} \\
y_i^l \leftarrow \gamma \, \hat{a}_i^l + \beta \equiv LN_{\gamma, \beta}{(a_i^l)}
$$

- H： 某层中的隐层单元数

## 3. Weight Normalization -- TODO

------

## QA

### 0. 什么是 ICS？

ICS 指的是在训练过程中由于网络参数的变化而导致各层网络的输入分布发生了变化， 而深层网络通过层层叠加，高层的输入分布变化会非常剧烈，这就需要高层不断去重新适应底层的参数更新。而为了训好模型，我们需要非常谨慎的去设定学习率， 初始化权重等。

从数学角度来看，它指的是源空间与目标空间的条件概率是一致的，但是其边缘概率不同。 即对所有 $x \in X, P_s(Y|X=x) = P_t(Y|X=x)$ ， 但是 $P_s(X) \neq P_t(X)$ 。

其实，通俗理解来说，就是对于神经网络的各层输出，由于经过了一系列的隐层操作，其分布显然与各层对应的输入数据分布不同，且这种差距会随着网络深度的增加而加大。而在训练过程中， 网络参数的变化会使得各层网络的输入分布发生变化，越深的网络，变化可能越大。

### 0. ICS 会导致什么问题？

- 上层参数需要不断适应新的输入数据分布，降低学习速度
- 下层输入的变化可能趋向于变大或变小，导致上层落入饱和区，使得学习过早停止。
- 每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。

### 1. BN 到底解决了什么问题？

- 解释1： 原论文说 BN 解决了 ICS 问题，但后续有论文推翻了这个结论，参见：《How Does Batch Normalization Help Optimization》

- 解释2： 为了防止梯度消失，这也很好理解， BN 将激活函数的输入数据压缩在 N(0,1) 空间内，的确能够很大程度上减轻梯度消失问题。

- 解释3： 来源于《How Does Batch Normalization Help Optimization》 ， BN 使得优化空间更加平滑，具体来说，BN实际上是改变了损失函数的 lipschitz 性质， 使得梯度的改变变得很轻微，这使得我们可以采用更大的步长且仍然能够保持对实际梯度方向的精确估计。

  通俗来讲， 不进行BN， 损失函数不仅仅非凸且趋向于坑洼，平坦区域和极小值，这使得优化算法极不稳定，使得模型对学习率的选择和初始化方式极为敏感，而BN大大减少了这几种情况发生。

我个人更倾向于第三种解释。

### 2. BN 的优点与缺点

- BN 的优点：

  > - 加速网络训练（缓解梯度消失，支持更大的学习率）
  > - 抑制过拟合
  > - 降低了**参数初始化**的要求。

- BN 的缺点：

  > - **对 batch size的要求较高**。这是因为如果 batch size 过小，无法估计出全局的样本分布
  > - **训练和预测时有些差别。**训练时一个 batch 之间进行 Normalization， 预测时需要依靠训练时获得的 均值和方差来进行预测。

### 3. 为何训练时不采用移动平均？

参见： 《Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models》

- 使用 BN 的目的就是为了保证每批数据的分布稳定，使用全局统计量反而违背了这个初衷；
- BN 的作者认为在训练时采用移动平均可能会与梯度优化存在冲突；

### 4. BN 与 LN 的区别是什么？

LN 对层进行 Normalization ， BN 对 batch 进行 Normalization。 LN 拜托了对 batch size 的依赖， 在 NLP 领域，使用极为广泛， 基本不用 BN。 

我个人的认为是， BN 是对batch进行操作，然而， 语言的复杂度使得 一个 batch 的数据对于全局的数据分布估计极为不准，这使得 BN 的效果变得很差。

### 5. 什么时候使用 BN 或 LN？

- 一般只在深层网络中使用， 比如在深层的 Transformer 中， LN 就起到了很关键的作用。 
- BN 与 LN 应用在非线性映射前效果更佳。
- 当你发现你的网络训练速度慢，梯度消失，爆炸等问题时， 不妨考虑加入 BN 或 LN 来试试。
- 使用 BN 时， 对 batch size 要求较高， 且在训练前需要对数据进行 shuffle。

### 6. BN 在何处做？

BN 可以对**第 L 层激活函数输出值**或**对第 L层激活函数的输入值**进行 Normalization， 对于两个不同的位置，不少研究已经表明：**放在第 L 层激活函数输出值会更好**。

### 7. 为什么要归一化？

- 归一化的确可以避免一些不必要的数值问题。
- 为了程序运行时收敛加快。 
- 统一量纲。样本数据的评价标准不一样，需要对其量纲化，统一评价标准。这算是应用层面的需求。
- 避免神经元饱和。啥意思？就是当神经元的激活在接近 0 或者 1 时会饱和，在这些区域，梯度几乎为 0，这样，在反向传播过程中，局部梯度就会接近 0，这会有效地“杀死”梯度。
- 保证输出数据中数值小的不被吞食。 







## Reference

[1] Batch Normalization

[2] Layer Normalization

[3] How Does Batch Normalization Help Optimization

[4]  Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models

[深度学习中 Batch Normalization为什么效果好？](https://www.zhihu.com/question/38102762/answer/85238569) - 知乎 

[深度学习中的Normalization模型](<http://www.cvmart.net/community/article/detail/368>)