# RNN 

RNN是一种节点定向链接成环的人工神经网络，是一种**反馈神经网络**，RNN利用内部的记忆来处理任意时序的输入序列，并且在其处理单元之间既有内部的反馈链接又有前馈链接，使得RNN更容易处理不分段的文本。

tags: 深度学习

---

[TOC]

## 基础相关

- [RNN ： NLP中最常见的神经网络单元](https://zhuanlan.zhihu.com/p/44106750)
- [LSTM：RNN最常用的变体](https://zhuanlan.zhihu.com/p/44124492)
- [RNN 的梯度消失问题](https://zhuanlan.zhihu.com/p/44163528)
- [RNN vs LSTM vs GRU -- 该选哪个？](https://zhuanlan.zhihu.com/p/55386469)

## QA

### 1. RNN 中为何会出现梯度消失，梯度爆炸问题

[RNN 的梯度消失问题](https://zhuanlan.zhihu.com/p/44163528)

因此， RNN 的梯度消失，梯度爆炸问题在于： 
$$
\prod_{j=k+1}^t \frac{\delta S_j}{\delta S_{j-1}} = \prod_{j=k+1}^t tanh' W_s
$$

### 2. Relu 能否作为RNN的激活函数

答案是可以，但会产生一些问题：

> - 换成 Relu 可能使得输出值变得特别大，从而产生溢出
> - 换成Relu 也不能解决梯度消失，梯度爆炸问题，因为还有 $W_s$ 连乘的存在（如1中公式）

为什么 CNN 和前馈神经网络采用Relu 就能解决梯度消失，梯度爆炸问题？

> 因为CNN 或 FNN 中各层的 W 并不相同， 且初始化时是独立同分布的，一定程度熵可以抵消。
>
> 而 RNN 中各层矩阵 $W_s$ 是一样的。

### 3. 推导 LSTM

关键在于三个门， 三个状态。 其中三个门的公式基本一样

- 遗忘门： 决定上一时刻的 $c_{t-1}$ 多少保留到当前时刻 $c_t$：

$$
f_t = \sigma{(W_f \cdot [h_{t-1},x_t] + b_f)} 
$$

- 输入门：决定当前时刻输入 $x_t$ 有多少保存到当前时刻 $c_t$：
  $$
  i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)
  $$

- 输出门：控制当前时刻的 $c_t$ 有多少信息作为当前时刻的 $h_t$:
  $$
   o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o) 
  $$

- 当前输入的状态 $\tilde{c_t}$： 将上一时刻的 $h_{t-1}$ 与当前时刻的 $x_t$ 融合：
  $$
  \tilde{c}_t=\tanh(W_c\cdot[h_{t-1},x_t]+b_c)
  $$

- 当前时刻的状态 $c_t$： 将 $x_t$ ， $\tilde{c_t}$ , $h_{t-1}$ 融合：
  $$
   c_t=f_t \circ c_{t-1}+i_t \circ \tilde{c}_t 
  $$

- 当前时刻的输出 $h_t$：从 $c_t$ 中分出一部分信息：
  $$
  h_t=o_t\circ \tanh(c_t)
  $$



### 3.1.LSTM 几个门介绍

#### 遗忘门

- **作用对象**:细胞状态

- **作用**:将细胞状态中的信息选择性的遗忘

  在语言模型的例子中，来基于已经看到的预测下一词，在这个问题中，细胞状态可能包含当前主语的类别，因此，正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语

- **第一个阶段**：忘记阶段

  这个阶段主要是对上一个节点传进来的输入进行选择性忘记，简单说就是**忘记不重要的，记住重要的**

#### 输入层门

- **作用对象**:细胞状态

- **作用**:将新的信息选择性的记录到细胞状态中

  在语言模型例子中，我们希望增加新的主语的类别到细胞状态中，来代替旧的需要忘记的主语

- **第二个阶段**：选择记忆阶段

  这个阶段将这个阶段的输入有选择的进行记忆。主要是会对输入xt进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。

#### 输出层门

- **作用对象**：隐层ht

  在语言模型的例子中，因为他就看到了一个代词，可能需要输出与一个动词相关的信息，例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化

- **第三阶段**:输出阶段

  这个阶段决定哪些会被当成当前状态的输出

### 4. LSTM 长短记忆机制

长短记忆机制主要通过 **输入门** 与 **遗忘门** 来实现：

- 如果当前信息$x_t$不重要， 则**输入门**相应维度接近于 0， 当前的信息就几乎不融入进入 $\tilde{c_t}$ 。反之， 输入门相应维度接近于 1， 当前信息实现很好的融入。
- 如果之前的信息 $c_{t-1}$ 不重要，则遗忘门相应维度接近于 0， 过去的信息就几乎不融入进  $c_t$。 反之，亦然。

### 5. LSTM的门机制为何选择 sigmoid 作为激活函数？

值得一提的是， 目前几乎所有主流的门控机制中，门控单元的选择均使用 sigmoid 。

- sigmoid 的饱和性： 十分符合 门控 的效果
- 值域在 (0,1)， 当输入较大或较小时，输出会接近1 或 0， 从而保证门的开或关。

### 6.  融合信息时为何选择 tanh？

- 值域为 (-1, 1)， 这样会带来两个好处：

  > - 与大多数情景下特征分布以 0 为中心相吻合。（激活函数一章中有提到这点特性的重要性）
  > - 可以避免前向传播时的数值溢出问题(主要是上溢)

- tanh 在 0 附近有较大的梯度，模型收敛更快

### 7. 计算资源有限的情况下有没有什么优化方法？

- 采用 Hard gate
- 采用 GRU

### 8. 推导一下 GRU 

两个门，两个状态

- 更新门：控制前一时刻状态信息与当前输入融合
  $$
  z_t = \sigma (W_z x_t + U_z h_{t-1}) 
  $$

- 重置门：控制前一时刻状态信息
  $$
  r_t = \sigma(W_r x_t + U_r h_{t-1})
  $$

- 当前输入的信息融入 $h_t'$： 
  $$
  h_t' = tanh(Wx_t + r_t \odot Uh_{t-1}) \\
  $$

- 当前时刻的状态：
  $$
  h_t = z_t \odot h_{t-1} + (1-z_t) \odot h_{t-1}'
  $$




### 9. LSTM 与 GRU 之间的关系

- GRU 认为**遗忘门**与**输入门**功能有一定的的重合，认为之前的信息 与 当前的输入信息是此消彼长的关系，因此将二者合并成一个门： 更新门。
- 合并了记忆状态 c 与隐藏状态 h
- 采用**重置门**代替了**输出门**

### 10 为何RNN 会有梯度消失现象，推一下？

[RNN 的梯度消失问题](https://zhuanlan.zhihu.com/p/44163528)

### 11. LSTM 与 GRU 区别

- LSTM 中的单元状态 c 与 GRU 中的 h 类似，但 GRU 去掉了 h 这个状态，即最后的输出不再进行调节，那么也就不需要输出门了
- 在产生新的全局状态时， LSTM 采用 输入门+遗忘门 的方式， 而 GRU 只采用更新门来控制
- 更新门起到了遗忘门的作用， 重置门起到了输入门的作用。

### 12. 为何 RNN 训练时 loss 波动很大

由于RNN特有的memory会影响后期其他的RNN的特点，梯度时大时小，learning rate没法个性化的调整，导致RNN在train的过程中，Loss会震荡起伏。

为了解决RNN的这个问题，在训练的时候，可以设置临界值，当梯度大于某个临界值，直接截断，用这个临界值作为梯度的大小，防止大幅震荡。

### 13. LSTM 中的激活函数选择

LSTM， 门的激活函数选择 Sigmoid， 而在生成 c 时采用 tanh。

- Sigmoid函数的输出在0～1之间，符合门控的物理定义。且当输入较大或较小时，其输出会非常接近1或0，从而保证该门开或关。
- 在生成候选记忆时，使用Tanh函数，是因为其输出在−1～1之间，这与大多数场景下特征分布是0中心的吻合。此外，Tanh函数在输入为0附近相比Sigmoid函数有更大的梯度，通常使模型收敛更快。