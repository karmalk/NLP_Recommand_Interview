# 梯度消失，梯度爆炸问题

tags: 深度学习

---

[TOC]

## 1. 梯度消失，梯度爆炸产生原因

请看我的这篇文章： [RNN 的梯度消失问题](https://zhuanlan.zhihu.com/p/44163528)， 讲的已经很清楚了。

总的来说，梯度消失，梯度爆炸问题本质上是由于随着深度的加深，受到权重信息，激活函数的影响，连乘机制所引发的一系列问题。

简单来说：在用**反向传播算法**计算**误差项**时每一层都需要乘以本层激活函数的导数:
$$
\delta^{(l)} = (W^{(l+1)})^T\delta^{(l+1)}\bigodot f'(u^{(l)})
$$
如果激活函数的导数的绝对值小于1，多次连乘之后误差项很快会衰减到接近于0，参数的梯度值由误差项计算得到，从而导致前面层的权重梯度接近于0，参数无法得到有效更新，便是梯度消失问题。如果激活函数导数的绝对值大于1，多次乘积之后权重值会趋向于非常大的值，从而导致梯度爆炸问题。

## 2. 解决方案

解决梯度消失，梯度爆炸问题，最终的目的是解决如何在深层网络上的优化问题，当然深层网络所带来的问题远不止梯度消失，爆炸问题。

对于普通的前馈网络来说，梯度消失意味着无法通过加深网络层次来改善神经网络的预测效果，因为无论如何加深网络，只有靠近输出的若干层才真正起到学习的作用，这使得循环神经网络模型很难学习到输入序列中的长距离依赖关系。

### 1. 采用 Relu 系激活函数

参考：[激活函数](./激活函数)

### 2. 合适的权重初始化

参考：[权重初始化](./权重初始化方案)

### 3.  残差结构

残差的方式，能使得深层的网络梯度通过跳级连接路径直接返回到浅层部分，使得网络无论多深都能将梯度进行有效的回传。

### 4. Batch Normalization， Layer Normalization

参考：[Normalization](./Trick - Normalization.md)

### 5. LSTM，ResNet

参考：[RNN 的梯度消失问题](https://zhuanlan.zhihu.com/p/44163528)， 讲的很清楚了。

### 6. 梯度裁剪 - 梯度爆炸

梯度爆炸问题可以通过梯度裁剪来缓解，即当梯度的范式大于某个给定值(设定的阈值)时，对梯度进行**等比收缩**

# 附：

### 退化问题

与过拟合不同，退化是指训练集和测试集上的误差都很大。实验证明，神经网络的训练误差和测试误差会随着层数的增加而增大。

### 局部极小值

神经网络的损失函数一般不是凸函数，因此，在训练时有陷入局部极小值的风险，梯度下降法只能保证收敛到梯度为零的点，但这个点不一定是局部极值点，更不能保证是全局最优值点。

### 鞍点

鞍点与局部极值点不同，鞍点是指梯度为0，但Hessian矩阵不定的带你。