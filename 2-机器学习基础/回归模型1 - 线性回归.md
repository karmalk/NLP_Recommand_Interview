# 机器学习： 线性回归

tags: machine-learning

---

[TOC]



## 1. 线性回归

### 1. 简介 

简单来说，线性回归算法就是**找到一条直线（一元线性回归）或一个平面（多元线性回归）能够根据输入的特征向量来更好的预测输出y的值。**

其本质含义在于 X 与 Y 是线性相关的。
$$
y = \theta_0 + \theta_1x_1 + \cdots  + \theta_px_p  = \theta^Tx
$$

### 2. 线性回归如何训练？

在线性回归中， 我们可以通过两种方法来求取参数 $\theta$ ， 一种是采用**正规方程**， 一种是采用**梯度下降方法**。 

#### 1. 损失函数

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2, \qquad  \\ 或 \\ 矩阵表示: J(\theta) = \frac{1}{2m} (X\theta-y)^T(X\theta - y)
$$

#### 2. 正规方程

我们使用 $J(\theta) $对 $\theta$ 求导， 得到：
$$
\frac{\delta J(\theta)}{\delta \theta} = 2 X^T(X\theta - y)
$$
令上式为0，我们可以得到 $ \theta$  的值为：
$$
\theta = (X^TX)^{-1}X^Ty
$$
我们可以直接通过矩阵运算来求出参数 $\theta$  的解。 而上式我们发现其涉及到了矩阵的可逆问题，**如果 $(X^TX)^{-1} $可逆，那么参数 $\theta$ 的解唯一**。 **如果不可逆， 则此时就无法使用正规方程的方法来解。** 

#### 3. 梯度下降法

我们可以采用批量梯度下降算法， 此时有：

$$
\theta_j = \theta_j - \alpha \frac{\delta}{\delta \theta_j} J(\theta) \\ 带入J(\theta) 得： \theta_j = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}  \\ 或矩阵表达：\theta_j = \theta_j + \alpha \frac{1}{m}(y-X\theta)^Tx_j
$$

#### 4. 两种方法的比较

- 梯度下降中需要选择适当的学习率 $\alpha $
- 梯度下降法中需要多次进行迭代，而正规方程只需要使用矩阵运算就可以完成
- 梯度下降算法对多特征适应性较好，能在特征数量很多时仍然工作良好， 而正规方程算法复杂度为 $O(n^3) $，所以**如果特征维度太高（特别是超过 10000 维），那么不宜再考虑该方法。**
- 正规方程中矩阵需要可逆。

## 2. 岭回归

岭回归本质上是 **线性回归 + L2 正则化**。
$$
\hat{h}_{\theta}(x) = h_{\theta}(x) + \lambda \sum_i w_i^2
$$

### 岭回归与线性回归

线性回归中通过正规方程得到的 w 的估计：
$$
\hat{w} = (X^TX)^{-1}X^Ty
$$
但是，当我们有 N 个样本，每个样本有 $x_i \in R^p$， 当 N < p 时， $X^TX$ 不可逆， 无法通过正规方程计算，容易造成过拟合。

岭回归通过在矩阵 $X^TX$ 上加一个 $\lambda I$ 来使得矩阵可逆， 此时的 w 的估计：
$$
\hat{w} = (X^TX + \lambda I)^{-1}X^Ty
$$
而岭回归本质上是对 $L(w)$  进行 L2 正则化， 此时的 $J(w)$ 表示为：
$$
\begin{align}
J(w) &= \sum_{i=1}^N ||w^Tx_i - y_i ||^2 + \lambda w^Tw \\
&= (w^TX^T - Y^T)(Xw - Y) + \lambda w^Tw \\
&= w^TX^TXw - 2w^TX^TY  + Y^TY + \lambda w^Tw \\
&= w^T(X^TX + \lambda I)w - 2w^TX^TY + Y^TY
\end{align}
$$
那么对 $w$ 的极大似然估计有：
$$
\hat{w} = argmax \, J(w) \\
\frac{\delta J(w)}{\delta w} = 2(X^TX + \lambda I)w - 2 X^TY = 0
$$
那么我们就解得：
$$
\hat{w} = (X^TX + \lambda I)^{-1}X^Ty
$$
因此说， 岭回归本质上是 **线性回归 + L2 正则化**， 从而达到抑制过拟合的效果。

## 3. Lasso 回归

Lasso 回归的本质是 **线性回归 + L1 正则化**。
$$
\hat{h}_{\theta}(x) = h_{\theta}(x) + \lambda \sum_i |w_i|
$$

## 4. ElasticNet 回归

ElasticNet 回归 本质上是线性回归 + L1正则化 + L2 正则化。



## LWR - 局部加权回归

在线性回归中， 由于最终拟合出来的曲线是一条直线，其拟合能力极为有限（也可以解释为线性回归所求的是具有最小均方误差的无偏估计），因此很容易造成欠拟合现象， 而针对这个问题，有人提出了局部线性回归(LWR)。

局部加权回归其思想很简单： 我们对一个输入 w 进行预测时，赋予了 x 周围点不同的权值，距离 x 越近，权重越高。整个学习过程中误差将会取决于 x 周围的误差，而不是整体的误差，这也就是局部一词的由来。

在LWR中， 其损失函数为：
$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^m w^{(i)} (h_\theta(x^{(i)}) - y^{(i)})^2 \\ 矩阵表示: J(\theta) = \frac{1}{2m} (X\theta-y)^TW(X\theta - y)
$$
此时，使用回归方程求得：

$$
\theta = (X^TWX)^{-1}X^TWy 
$$


而通常， $w^{(i)} $ 服从高斯分布， 在x周围指数型衰减;
$$
w^{(i)} = e^{- \frac{|x^{(i)} - x|}{2 k^2 }} 
$$

其中， **k 值越小，则靠近预测点的权重越大，而远离预测点的权重越小**。所以参数k的值决定了权重的大小。 

> - k越大权重的差距就越小，k越小权重的差距就很大，仅有局部的点参与进回归系数的求取，其他距离较远的权重都趋近于零。
> - 如果k去进入无穷大，所有的权重都趋近于1，W也就近似等于单位矩阵，局部加权线性回归变成标准的无偏差线性回归，会造成欠拟合的现象；
> - 当k很小的时候，距离较远的样本点无法参与回归参数的求取，会造成过拟合的现象。

---

## QA

### 1. 最小二乘法估计

$$
X = (x_1,  ..., x_N)^T \\
Y = (y_1, ..., y_N)^T
$$

$$
\begin{align}
L(w) &= \sum_{i=1}^N ||w^Tx_i - y_i ||^2 \\
&= \sum_{i=1}^N (w^Tx_i - y_i)^2 \\
&= \begin{pmatrix} w^Tx_1 - y_1 & ... & w^Tx_N - y_N  \end{pmatrix} \begin{pmatrix} w^Tx_1 - y_1 \\ ... \\ w^Tx_N - y_N  \end{pmatrix}
\end{align}
$$

其中有：
$$
\begin{align}
\begin{pmatrix} w^Tx_1 - y_1 & ... & w^Tx_N - y_N  \end{pmatrix} = w^T \begin{pmatrix} x_1  & ... & x_N  \end{pmatrix} - \begin{pmatrix} y_1  & ... & y_N  \end{pmatrix} &=  w^TX^T - Y^T
\end{align}
$$
$$
\begin{align}
\begin{pmatrix} w^Tx_1 - y_1 \\ ... \\ w^Tx_N - y_N  \end{pmatrix} = \begin{pmatrix} x_1 \\ ... \\ x_N  \end{pmatrix}w - \begin{pmatrix}  y_1 \\ ... \\  y_N  \end{pmatrix}  = Xw-Y
\end{align}
$$



那 么，最终就得到：
$$
\begin{align}
L(w) &=  (w^TX^T - Y^T)(Xw + Y) \\
&= w^TX^TXw - w^TX^TY - Y^TXw - Y^TY
\end{align}
$$
考虑到 $w^TX^TY$ 与 $Y^TXw$ 的结果其实都是一维实数且二者为转置，因此，二者的值相等， 那么就有：
$$
L(w) = w^TX^TXw - 2w^TX^TY - Y^TY
$$
那么就有：
$$
\hat{w} = argmin \, L(w) \\
\frac{\delta L(w)}{\delta w} = 2X^TXw - 2X^TY = 0
$$
从而就得到：
$$
w = (X^TX)^{-1}X^TY
$$



### 2. 最小二乘法的几何解释



### 3. 从概率角度看最小二乘法

最小二乘法隐藏的一个条件是： **误差服从正态分布。**

我们假设起始条件：
$$
X = (x_1, \cdots, x_N)^T \\
Y = (Y_1, \cdots, y_N)^T
$$
假设误差服从正态分布，那么则有：
$$
\varepsilon \sim N(0, \sigma^2) \\
$$
那么当噪声服从正态分布时，输出值也服从正态分布：
$$
y = f(w) + \varepsilon = w^Tx + \varepsilon
$$

$$
y | (x;w) \sim N(w^Tx, \sigma^2); \\
P(y|x;w) = \frac{1}{\sqrt{2\pi\sigma}} e^{-\frac{(y-w^Tx)^2}{2 \sigma^2}} \\
$$
极大似然过程推导：
$$
\begin{align}
L(w) &= log P(Y|X;w)  \\
&= log \prod_{i=1}^N P(y_i|x_i; w) \\
&= \sum_{i=1}^N log P(y_i | x_i; w) \\
&= \sum_{i=1}^N (log \frac{1}{\sqrt{2 \pi \sigma}} + log \, e^{-\frac{(y_i-w^Tx_i)^2}{2 \sigma^2}}) \\
&= \sum_{i=1}^N (log \frac{1}{\sqrt{2 \pi \sigma}}  -\frac{(y_i-w^Tx_i)^2}{2 \sigma^2}) \\
&:= \sum_{i=1}^N ( -(y_i-w^Tx_i)^2) \\
&= - \sum_{i=1}^N (y_i-w^Tx_i)^2
\end{align}
$$
我们最终是要用极大似然估计 $w$：
$$
\begin{align}
\hat{w} &= argmax_w L(w) \\
&= argmax_w - \sum_{i=1}^N (y_i-w^Tx_i)^2
\end{align}
$$


### 4. 推一下线性回归的反向传播

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 \\
\theta_j = \theta_j - \alpha \frac{\delta}{\delta \theta_j} J(\theta) \\ 带入J(\theta) 得： \theta_j = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}  \\ 或矩阵表达：\theta_j = \theta_j + \alpha \frac{1}{m}(y-X\theta)^Tx_j
$$

### 5. 从贝叶斯角度看线性回归

<https://www.bilibili.com/video/av31989606/?p=4>



### 6. 什么时候使用岭回归 ？

如果样本数据过少导致线性回归拟合较差，则考虑采用岭回归。如何输入特征的维度很高,而且是稀疏线性关系的话， 岭回归就不太合适,考虑使用Lasso回归。

### 7. 什么时候使用 L1 正则化？

**L1正则化(Lasso回归)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0**，从而增强模型的泛化能力 。对于高的特征数据,尤其是线性关系是稀疏的，就采用L1正则化(Lasso回归),或者是要在一堆特征里面找出主要的特征，那么L1正则化(Lasso回归)更是首选了。



