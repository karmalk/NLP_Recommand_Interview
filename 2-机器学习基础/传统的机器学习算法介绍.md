#  一些常用的机器学习算法



## 逻辑回归

逻辑回归是假设数据服从伯努利分布，通过极大似然估计的方法，运用梯度下降算法作为求解方法求解参数，达到对数据进行二分类或者多分类(softmax多项逻辑回归)目的,本质上还是线性回归，属于广义线性回归，只是在特征的映射中加入了一层逻辑函数g(z)，即先把线性特征求和，然后使用函数g(z)作为假设函数来预测，g(z)可以将连续值映射到0和1 .g(z)就是sigmoid函数

**假设函数**
$$
g(z) = \frac{1}{1+e^{-z}}
$$

$$
h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}}
$$

**逻辑回归的损失函数**

采用对数似然作为损失函数(似然函数取对数)
$$
J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(\bar{y}^{i},y^{i}) = \frac{1}{m}\sum_{i=1}^{m}(-ylog(\bar{y}^{(i)})-(1-y^{(i)})log(1-\bar{y}^{(i)}))
$$
**似然函数**
$$
\prod_{i=1}^{N}[\pi(x_{i})]^{y_{i}}[1-\pi(x_{i})]^{1-y_{i}}
$$
对数似然函数
$$
l(w) = \sum_{i=1}^{M}[y_{i}log\pi(x_{i})+(1-y_{i})log(1-\pi(x_{i}))]
$$

$$
= \sum_{i=1}^{N}[y_{i}log\frac{\pi(x_{i})}{1-\pi(x_{i})}+log(1-\pi(x_i))]
$$

$$
= \sum_{i=1}^{N}(w*x_{i}-log(1+exp(w*x_{i})))
$$

求l(w)的极大值，使问题变成以对数似然为目标的最优化问题，采用梯度下降法求解

**如何分类**

y值是一个连续变量，划定一个阈值，y值大于这个阈值的是一类，小于这个阈值的是另一类

#### 逻辑回归使用对数损失而不是用平方损失的原因

在使用Sigmoid函数作为正样本的概率时，同时将平方损失作为损失函数，这时所构造出来的损失函数是非凸的，不容易求解，容易得到其**局部最优解**，而如果使用极大似然，其目标就是对数似然函数，该损失函数时关于未知参数的**高阶连续可导的凸函数**，便于求全局最优解

将极大似然函数取对数后等同于对数损失，在逻辑回归这个模型下，对数损失函数的训练求解参数速度时比较快的，和sigmoid函数本身的梯度无关，这样更新的速度是可以自始至终都比较稳定，而如果使用平方损失函数，梯度更新的速度和sigmoid函数本身的梯度时很相关的，sigmoid函数在它定义域内的梯度都不大于0.25，这样训练会特别慢

**优点：**形式简单，模型的可解释性强，模型效果在工程上可以接受，训练速度较快，资源占用小，方便输出结果调整，

**缺点：**准确率不是太高，很难处理数据不平衡问题，处理非线性数据比较麻烦，逻辑回归其本身无法筛选特征。

### 逻辑回归常用的优化方法有哪些

- **一阶方法**
  - 梯度下降，随机梯度下降，mini随机梯度下降。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。
- **二阶方法**：**牛顿法**，**拟牛顿法**
  - **牛顿法** 其实就是通过切线与X轴的交点不断更新切线的位置，知道达到曲线与X轴的交点得到方程解。在实际应用中我们因为常常要求解决**凸优化**问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，也就引出了Hessian矩阵的概念，就是x的**二阶导数矩阵**
  - **缺点**：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定下降，严重时甚至会失败。还有就是牛顿法要求函数一定时二阶可导。最重要的时计算Hessian矩阵的逆复杂度很大。
  - **拟牛顿法**：不用二阶偏导而是构造出Hessian矩阵的**近似正定对称矩阵**的方法成为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hessian的逆，BFGS(直接逼近Hessian矩阵)，L-BFGS(可以减少BFGS所需的存储空间)）

### 逻辑回归为什么要对特征进行离散化

- 非线性，逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；离散特征的增加和减少都很容易，易于模型的快速迭代。
- 速度快：稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
- 鲁棒性：离散化后的特征对异常数据具有很强的鲁棒性
- 方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N 个变量，进一步引入非线性，提升表达能力；
- 稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 
- 简化模型：特征离散化以后，起到了简化逻辑回归模型的作用，降低了模型过拟合的风险。

### 逻辑回归模型的目标函数中增大L1正则化会时什么结果

所有的参数w都会变成0.



## SVM 

支持向量机是一个二分类模型，它的基本模型定义为特征空间上的**间隔最大的线性分类器**，因为支持向量机引入了核技巧，这使它成为实质上的非线性分类器，学习策略为**最大化分类间隔**，最终可形式化为凸二次规划问题求解，也等价于正则化的合页损失函数的最小化问题，支持向量机的学习算法是**求解凸二次规划的最优化算法**。

支持向量机的学习方法包含从简至繁的模型，包括由**线性可分支持向量机**，**线性支持向量机**，**非线性支持向量机**

- 当训练数据线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机，也叫硬间隔支持向量机
- 当训练数据近似可分时，通过软间隔最大化，也学习一个线性分类器，即线性支持向量机，也叫软间隔支持向量机。
- 当训练数据线性不可分时，通过使用核函数，以及软间隔最大化，学习非线性支持向量机

硬间隔和软间隔最大的区别在于是否引入了松弛变量

### SVM的损失函数

SVM的损失函数如下，第一项成为**经验风险(Hinge loss)**,度量了模型对训练数据的拟合程度；第二项成为**结构风险(正则化项)**，度量了模型自身的复杂度。正则化项削减了假设空间，从而**降低过拟合风险**。$\lambda$是个可调节的超参数，用于调整**经验风险和结构风险**的相对权重。
$$
min_{w,b}\frac{1}{m}\sum_{i=1}^{m}max(0,1-y_{i}(w^{T}\phi(x_{i}+b))+\frac{\lambda}{2}||w||^{2}
$$
损失函数一般为Hing loss(合页损失/铰链损失)
$$
J(\theta)=\frac{1}{2}||\theta||^{2}+C\sum_{i}max(0,1-y_{i}(\theta^{T}x_{i}+b))
$$
### 核方法

当输入空间为欧式空间或者离散集合，特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过核方法可以学习非线性支持向量机 ，等价于在高维的特征空间上学习线性支持向量机。

核方法是比SVM更为一般的机器学习方法，可以用到PCA，K-means多种模型中。

### 核函数

**核函数**：核函数隐含着一个从低维空间到高维空间的映射，这个映射可以把低维空间中线性不可分的两类点变成线性可分的                    

核函数包括，线性核 高斯(RBF)核，多项式核，sigmoid核函数等等

- 线性核
  - $x_{i}^{T}x_{j}$
  - 有高效实现，不易过拟合
  - 无法解决非线性问题
- 多项式核
  - $(\beta x_{i}^{T}x_{j} + \theta)^{n}$
  - 比线性核更一般，n直接描述了被映射空间的复杂度
  - 参数多，当n很大时，会导致计算不稳定
- RBF(高斯)核
  - $exp(-\frac{||x_{i}-x_{j}||^{2}}{2\sigma^{2}})$
  - 只有一个参数，没有计算不稳定问题
  - 计算慢，过拟合风险大

当样本的特征很多且维数很高时可以考虑用SVM的**线性核函数**

当样本量较多，特征较少时，一般手动进行特征的组合在使用SVM的**线性核函数**；

当样本维度不高且数量较少时，且不知道改用什么核函数时一般有限使用**高斯核函数**，因为高斯核函数为一种局部性较强的核函数，无论对于大样本还是小样本都有较好的性能，且相对于**多项式核函数**有较少的参数

### SMO算法(SVM中的系数的求解)

SMO 算法是一种快速算法，**有多个拉格朗日乘子，每次只选择其中两个乘子做优化，其他因子被认为是常数**。其特点时不断将**原二次规划问题**分解为只有两个变量的**二次规划子问题**求解，并对于问题进行解析求解，直到所有变量都满足KKT条件为止，这样通过启发式的方法得到原始二次规划问题的最优解，因为子问题有解析解，所以每次计算子问题都很快

主要分为两个部分:

- 求解两个变量二次规划的解析方法
- 选择变量的启发式方法。

### SVR 

传统和回归模型的损失是计算模型输出f(x)和真实值y之间的差别，当且仅当f(x)=y时，损失才为0，但是SVR(支持向量回归)能容忍f(x)和y之间有一定的偏差$\varepsilon$,仅当f(x)和y之间的偏差大于$\varepsilon $才计算损失。
$$
min_{w,b}\frac{1}{m}\sum_{i=1}^{m}max(0,|y_{i}-(w^{T}\phi(x_{i}+b)|-\varepsilon)+\frac{\lambda}{2}w^{T}{w}
$$

### SVM的优缺点

优点:

- 由于SVM是一个**凸优化**问题，所以求得的解一定时全局最优解
- 不仅适用于线性问题还适用于非线性问题(使用核技巧)
- 拥有高维样本空间的数据也能用SVM，这是因为数据集的复杂度只取决于支持向量而不是数据集的维度，某种意义上避免了维数灾难
- 理论基础比较完善，可解释性强

缺点:

- 二次规划问题求解将涉及m(样本个数)阶矩阵的计算,因此SVM不适用于超大数据集(SMO算法可以缓解此问题)
- 当样本数量比较大时，效果通常不如神经网络。

### 为什么SVM对缺失数据敏感

这里的的缺失数据是指缺失某些特征数据，向量数据不完整，SVM没有处理缺失值的策略。而SVM希望样本再特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。

### 为什么要将求解SVM的原始问题转换为求对偶问题

- 首先我们有不等式约束方程，这就需要写成minmax形式去求得最优解，而这种形式对于x不能求导，所以需要转换为maxmin形式，这时候x就在里面，可以求解，可以对x进行求导，而为了满足这种对偶变换的成立，这就需要满足KKT条件，KKT条件是原始问题与对偶问题等价的必要条件，当原始问题是凸优化问题时，变为充要条件。
- 对偶问题将原始问题的不等式约束转换为对偶问题的等式约束。
- 方便核函数的引入，进而推广到非线性分类问题。
- 改变了问题的复杂度，由求特征向量w转换为求比例系数$\alpha$，在原始问题下，求解的复杂度与样本维数相关，即w的维度，在对偶问题下，之和样本数量有关。

### 讲一下SVM中松弛变量$\varepsilon$和惩罚因子C

松弛变量和惩罚因此是为了把线性可分SVM拓展到**线性近似可分**或者**线性不可分**SVM的，只有被决策面分类错误的点(线性不可分)才会有松弛变量，然后惩罚因子是对线性不可分点的惩罚。

**增大惩罚因子，模型泛化性能变弱，惩罚因子无穷大时，退化为线性可分SVM(硬间隔)**;

**减少惩罚因子，模型泛化性能变好**

- 并非所有的样本点都有一个松弛变量与其对应。实际上只有离群点才有，或者说，所有没离群的点松弛变量都等于0，
- 松弛变量的值实际上标示除了对应的点到底离群有多远，值越大，点就越远。
- 惩罚因子C决定了你有多重视离群点带来的损失，显然当所有离群点的松弛变量的和一定时，你定的C 越大，对目标函数的损失也就越大，此时就暗示着你非常不愿意放弃这些离群点，最极端的情况是你把C定为无限大，这样只要稍有一个点离群，目标函数的值马上变为无限大，马上让问题变成无解，这就退化成了硬间隔问题。当C无穷大时，为了最小化损失函数，只能使松弛变量无穷小(趋近于0),等价于线性可分SVM。



### SVM中的函数间隔和几何间隔分别都是什么.

对于训练样本$x_{i}$,超平面(w,b),点距离超平面越远表示预测的置信度越高，另外，真实标签是$y_{i}$,对于二分类问题，取值为1或者-1,函数间隔可以反映分类的正确性和确信度，公式
$$
y_{i}(wx_{i}+b)
$$
对于一个训练集来说，函数距离越小，超平面越好，但是函数距离有一个问题，选择分离超平面时，只有函数间隔还不够，因为只要成比例的改变w和b，超平面并没有改变，但函数间隔却成为原来的2倍，这样的结果并不是很好，因此对上式进行归一化，对分离超平面的法向量w加些约束，||w|| = 1,使得间隔是确定的，这样就得到几何间隔
$$
y_{i}(\frac{w}{||w||}x_{i} + \frac{b}{||w||})
$$

## RandomForest 随机森林

随机森林是一个包含多个决策树的分类器，属于bagging类的集成学习算法，通过采用bootstrap(自主随机采样),使用多个决策树作为基准分类器构建的森林，因为属于bagging类的集成算法，所以这些基准决策树模型之间没有关联，相互独立。随机森林是由多棵决策树构成得，对于每棵树，他们使用的训练集是采用又放回的方式从总的训练集中采样出来的，而在训练每棵树的结点时，使用的特征是从所有特征中按照一定比例**随机的无放回**的方式抽取的

**特点**

- 当分类资料具有很多种时，可以产生高准确度的分类器
- 当分类数据为不平衡数据时，随机森林可以平衡误差
- 随机森林在大数据集上表现很好
- 可以评估在分类问题上的各个特征的重要程度。

**构造方法**：随机森林的建立基本上是由**随机采样**和**完全分裂**两部分组成的。

**随机采样**

随机森林对输入数据进行行和列的采样，但两种采样的方法有所不同，行采样采用的方式是有放回的采样，即在采样得到的样本集合中，可能会有重复的样本，假设输入样本为N个，那么采样的样本也为N个，

**完全分裂**

在形成决策树的过程中，决策树的每个结点都要按照完全分裂的方式进行分裂，直到结点不能再分裂，采用这种方式建立出的决策树的某一个叶子结点要么是无法继续分裂的，要么里面的所有样本都是指向同一个分类器。

**优点**

- 能够处理高维度的数据，并且不用做特征选择，因为特征子集是随机选择的。
- 再训练完成后，可以得出特征重要性
- 再创建随机森林的时候，采用了随机采样，对泛化误差采用的是无偏估计，模型泛化能力强
- 随机森林由oob(袋外数据)，不需要单独换分交叉验证集
- 训练时每个树都是相对独立的，训练速度快，可以并行化
- 对缺失值不敏感
- 相对于Boosting系列的算法，RF实现比较简单

**缺点**

- 随机森林再某些噪音较大的数据集上会容易过拟合
- 对于由不同取值属性的数据，取值划分较多的属性会对随机森林产生更大的影响，影响模型的拟合效果。

## 决策树

决策树是一种自上而下，对样本数据进行树形分类的过程，由结点和有向边组成，结点分为**内部节点**和**叶结点**，其中每个內部结点表示一个特征和属性，叶结点表示类别，从顶部根结点开始，所有样本聚在一起。经过根结点得划分，样本被分到不同得子结点中，再根据子结点得特征进一步划分，直至所有得样本都被归到一个类别中(即叶结点)中。

其学习构建本质上就是从训练集中归纳出一组分类规则，使他与训练数据矛盾较小得同时具有较强得泛化性能，从另一个角度看，学习也是基于训练数据集估计**条件概率模型**，决策树得损失函数通常是正则化得极大似然函数，学习得策略是以损失函数为目标函数得最小化。

这个最小化问题是一个NP(多项式复杂程度的非确定性问题)完全问题，通常采用启发式算法，来近似求解这一最优化问题，得到的决策树是次最优的，

- 特征选择
- 模型生成
- 决策树剪枝

从总体来看，这三个步骤就是一个递归选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类过程，这个过程就是划分特征空间，构建决策树的过程。

### 都有哪些决策树

- ID3 通过使用信息增益来对样本进行划分
  - ID3决策树优先选择信息增益大的属性进行划分，信息增益反映的是给定条件以后不确定性较少的程度，特征取值越多，就意味着确定性越高，也就是条件熵越小，信息增益越大，但是这样的分裂结点有一个很大的缺点，就是当一个属性可取值的数目比较多时，此时它的信息增益会特别高，ID3会认为这个属性很适合划分，但实际情况下较多属性的取值会使模型泛化能力较差。
- C4.5 通过使用信息增益比来对样本进行划分
  - C4.5为了解决这样的问题，也就是为了对ID3进行优化，引入了信息增益比这个概念，通过引入信息增益比，一定程度上对取值较多的特征进行惩罚，避免ID３出现过拟合的现象，提升决策树的泛化能力，
- CART 通过基尼系数来对样本进行划分

### ID3和C4.5的区别

- ID3只能处理离散型变量，而C4.5和CART都可以处理连续变量
  - C4.5处理连续型变量时，通过对数据排序后找到类别不同的分割线作为切分点，根据切分点把连续属性转换为布尔型，从而将连续型变量转换为多个取值区间的离散型变量，而对于CART，由于其构建时每次都会对特征进行二值划分，，因此可以很好的适用于连续变量
  - ID3和C4.5只能适用于分类任务，而CART可以适用于回归任务(回归树使用最小平方误差准则)
  - ID3对样本缺失值特别敏感，而C4.5和CART可以对缺失值进行不同方式的处理
  - ID3和C4.5可以在每个结点上产生出多交叉分支，且每个特征在层级之间不会复用，而CART每个结点只会产生两个分支，因此最后会形成一个二叉树，且每个特征可以被重复利用。
  - ID3和C4.5通过剪枝来权衡树的准确性和泛化能力，而CART直接利用全部数据



### 决策树如何剪枝 (预剪枝和后剪枝的区别)

- **预剪枝**：再每一次实际对结点进行进一步划分之前，先采用**验证集的数据来验证**如果划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点
- **后剪枝**：后剪枝则是先从训练集生成一颗完整的决策树，然后**自底向上地对非叶结点进行考察**，若将该结点对应得子树替换为叶结点能带来泛化性能得提升，则将该子树替换为叶结点。

## AdaBoost

AdaBoost算法也是一种集成学习算法，用于二分类问题，是Boosting算法的一种实现。他用多个弱分类器的线性组合来预测，训练时重点关注错分的样本，准确率高的弱分类器权重大。AdaBoost算法的全称是自适应，它用弱分类器的线性组合来构造强分类器。弱分类器的性能不用太好，仅比随机猜测强，依靠它们可以构造出一个非常准确的强分类器。强分类器的计算公式为：
$$
F(x) = \sum_{m=1}^{M}\alpha_{m}G_{m}(x)
$$
其中x是输入向量，F(x)是强分类器，$G_m(x)$是弱分类器，$\alpha_m$是弱分类器的权重，
M为弱分类器的数量，弱分类器的输出值为+1或者-1，分别对应正样本和负样本，分类时的判定规则为sgn(F(x))

AdaBoost算法的特点是通过迭代每次学习一个基本分类器，每次迭代中，

**提高**那些被前一轮分类器**错误分类数据样本**的**权值**，**降低**那些被**正确分类的数据样本**的**权值**

最后，AdaBoost将基本分类器的线性组合作为强分类器，

其中给**分类误差率小**的**基本分类器**以**大**的权值，给**分类误差率大**的**基本分类器**以**小**的权值

AdaBoost的训练误差分析表明，AdaBoost的每次迭代可以减少它在训练数据集上的分类误差率，着说明了它作为提升方法的有效性。

AdaBoost算法的一个解释是该算法实际是前向分布算法的一个实现，在这个方法里，模型是加法模型，损失函数是指数损失，算法是前向分布算法，每一步中极小化损失函数:
$$
(\beta_m,\gamma_m) = argmin_{\beta,\gamma}\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i) + \beta b(x_i;\gamma))
$$
得到参数$\beta_m , \gamma_m$

## EM算法

EM算法是一种**迭代算法**,用于含有隐变量的概率模型参数的**最大似然估计**，或**极大后验概率估计**。含有隐变量的概率模型的数据表示为$P(Y,Z|\theta)$，这里，Y是观测变量的数据，Z是隐变量的数据，$\theta$是模型参数，EM算法通过迭代求解观测数据的对数似然函数$L(\theta)=logP(Y|\theta)$的极大化，实现极大似然估计。EM算法的每次迭代由两步组成

- E 步，求期望 求$logP(Y,Z|\theta)$关于$P(Z|Y,\theta^{(i)})$的期望
  - $Q(\theta,\theta^{(i)}) = \sum_{Z}logP(Y,Z|\theta)P(Z|Y,\theta^{(i)})$
  - Q函数，这里$\theta^{i}$是参数的现估计值。
- M步，求极大化极大化Q函数得到的参数的新估计值
  - $\theta^{(i+1)} = argmax_{\theta}Q(\theta,\theta^{(i)})$
  - 在构建具体的EM算法时，重要的是定义Q函数，每次迭代中，EM算法通过极大化Q函数来增大对数似然函数$L(\theta)$

EM 算法在每次迭代后均提高观测数据的似然函数值，即
$$
P(Y|\theta^{(i+1)}) >= P(Y|\theta^{(i+1)})
$$
在一般条件下，EM算法是收敛的，但不能保证收敛倒全局最优。

EM算法主要应用于含有**隐变量**的概率模型的学习，**高斯混合模型的参数估计**是EM算法的一个重要应用，

EM算法还可以解释为F函数的极大-极大算法，EM算法有很多变形，如GEM算法，GEM算法的特点是每次迭代增加F函数值(并不一定是极大化F函数)，从而增加似然函数值。

。。。。待补

## 隐马尔可夫模型(生成模型)

隐马尔可夫模型是关于时序的概率模型(可用于标注问题，状态对应着标记，标注问题是给定观测序列预测其对应的标记序列),描述由一个隐藏的马尔可夫链随机生成不可观测的状态的序列，再由各个状态随机生成一个观测而产生观测的序列的过程。

隐马尔可夫模型由**初始状态概率向量**$\pi$,**状态转移概率矩阵**A和**观测概率矩阵**B决定，因此，隐马尔可夫模型可以写成$\lambda = (A,B,\pi)$

隐马尔可夫模型是一个生成模型，表示**状态序列**和**观测序列**的联合分布，但状态序列是隐藏的，不可观测的。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  

三个主要问题

- **概率计算问题**
  - 给定模型$\lambda = (A,B,\pi)$和观测序列$O = (o_1,o_2,...,o_T)$，计算再模型$\lambda$下观测序列O出现的概率$P(O|\lambda)$,**前向-后向算法**是通过递推地计算**前向-后向概率**可以高校地进行隐马尔可夫模型的概率计算。
- **学习问题**
  - 已知观测序列$O = (o_1,o_2,...,o_T)$,估计模型$\lambda = (A,B,\pi)$参数，使得在该模型下观测序列概率$P(O|\lambda)$最大。即用极大似然估计的方法估计参数。**Baum-Welch(BW)**算法，也就是EM算法可以高效地对隐马尔可夫模型进行训练，是一种**非监督学习算法**。
- **预测问题**
  -  已知模型$\lambda = (A,B,\pi)$和观测序列$O = (o_1,o_2,...,o_T)$,求对给定观测序列条件概率P(I|O)最大的状态序列$I = (i_1,i_2,i_3,...,i_T)$。**维特比**算法应用动态规划高校地求解最优路径，即概率最大地状态序列。

## KNN 最近邻

KNN算法将样本分到离他最相似的样本所属的类，算法本质上使用了模板匹配的思想。要确定一个样本的类别，可以计算它与所有训练样本的距离，然后找出和该样本最接近的k个样本，统计这些样本的类别进行投票，票数最多的那个类就是分类结果。

由于需要计算样本间的距离，因此需要依赖距离定义，常用的由**欧式距离**，**马氏距离**和**巴氏距离**。另外，还可以通过学习得到距离函数，这就是距离度量学习。

KNN算法是一种判别模型，即支持分类问题，也支持回归问题，是一种非线性模型，它天然的支持多分类问题。KNN算法没有训练过程，是一种基于实例的算法。



## K means聚类算法

K均值算法是一种无监督的聚类算法，算法将每个样本分配到离他最近的那个类中心所代表的类，而类中心的确定又依赖于样本的分配方案。在实现时，先随机初始化每个类的类中心，然后计算样本与每个类的中心的距离，将其分配到最近的那个类，然后根据这种分配方案重新计算每个类的中心。这也是一种分界点优化的策略。

与KNN算法一样，这里也依赖于样本之间的距离，因此需要定义距离的计算方式，最常用的是欧式距离，也可以采用其他距离定义，算法在实现时要考虑下面几个问题：

- **类中心向量的初始化**：一般采用随机初始化。最简单的是Forgy算法，它从样本集中随机选择K个样本作为初始类中心。第二种方案时随机划分，它将所有样本随机分配给K个类中的一个，然后按这种分配方案计算各个类的类中心向量。
- **参数K的设定**；可以根据先验知识人工指定一个值，或者由算法自己确定
- **迭代终止的判定规则**。一般做法是计算本次迭代后的类中心和上一次迭代时的类中心之间的距离，如果小于指定阈值，则算法终止。