# 机器学习： 逻辑回归

tags: machine-learning

---





## logistic回归简介

**逻辑回归是假设数据服从伯努利分布，通过极大似然估计的方法，运用梯度下降算法作为求解方法求解参数，达到对数据进行二分类或者多分类(softmax多项逻辑回归)的目的，本质上还是线性回归，属于广义线性回归，只是在特征的映射中加入了一层逻辑函数g(z)，即先把线性特征求和，然后使用函数g(z)作为假设函数来预测，g(z)可以将连续值映射到0和1，g(z)就是sigmoid函数。**

logistic回归用于解决的是分类问题，**其基本思想是：根据现有数据对分类边界线建立回归公式,以此进行分类。**也就是说，logistic 回归不是对所有数据点进行拟合，而是要对**数据之间的分界线**进行拟合。

- 逻辑回归的本质： 极大似然估计，广义线性回归
- 逻辑回归的激活函数：Sigmoid
- 逻辑回归的代价函数：交叉熵

## Logistic 回归的数学表达

$$
h_\theta(x) = sigmoid(\theta^T X)  = \frac{1}{1 + e^{-\theta^T X}}
$$
##  如何求解最优的 $\theta$

首先，我们依旧是要找到一个合适的损失函数，在Logistic回归中的损失函数为：
$$
Cost(h_{\theta}(x),y) = 
\begin{cases} -log(h_{\theta(x)}) & if \, y = 1\\
-log(1-h_{\theta(x)}) & if \, y = 0
\end{cases}
$$

$$
J(\theta) =    - \frac{1}{m}   \left[  \sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)}))   + (1-y^{(i)}) log(1 - h_\theta(x^{(i)}))             \right]
$$
我们最终给它加一个正则化项：
$$
J(\theta) =    - \frac{1}{m}   \left[  \sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)}))   + (1-y^{(i)}) log(1 - h_\theta(x^{(i)}))             \right] + \frac{\lambda}{2m} \sum_{j=1}^{m}\theta_j^2
$$
最后，我们要求最优参数的话，依旧是使用梯度下降算法来获取$J(\theta)$ 的最小值时对应的参数。

---

## 常见问题

### 1. 逻辑回归与线性回归

- 逻辑回归处理分类问题，线性回归处理回归问题

- 线性回归的拟合函数本质上是对 **输出变量 y 的拟合**， 而逻辑回归的拟合函数是对 **label 为1的样本的概率的拟合**。
  $$
  线性回归：f(x)=\theta ^{T}x \\
  逻辑回归：f(x)=P(y=1|x;\theta )=g(\theta ^{T}x)， \quad g(z)=\frac{1}{1+e^{-z}}
  $$

- 线性回归其参数计算方式为**最小二乘法**， 逻辑回归其参数更新方式为**极大似然估计**。

- 线性回归更容易受到异常值的影响， 而LR对异常值有较好的稳定性。

### 2. 推导一下 LR

- sigmoid :
  $$
  g(z) = \frac{1}{1+e^{-z}} \\
  g'(z) = g(z)(1-g(z))
  $$

- LR 的定义：
  $$
  h_{\theta}(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}
  $$

- LR 满足**伯努利分布：**
  $$
  P(Y=1|x; \theta) = h_{\theta}(x) \\
  P(Y=0|x; \theta)  = 1 - h_{\theta}(x) \\
  p(y|x; \theta) = (h_{\theta}(x))^y (1-h_{\theta}(x))^{1-y}
  $$

- **损失函数（极大似然）:**  对于训练数据集，特征数据 $x={x_1, ...x_m}$ 和其对应的分类标签 $y = {y_1,...y_m}$ ， 假设 m 个样本是相互独立的，那么极大似然函数为： 
  $$
  \begin{align}
  L(\theta) &= \prod_{i=1}^m p(y^{(i)}|x(i);\theta) \\ 
  &= \prod_{i=1}^m  (h_{\theta}(x^{(i)}))^{y^{(i)}} (1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}\\
  \end{align}
  $$
  那么它的 log 似然为：
  $$
  \begin{align}
  L(\theta) &= log L(\theta ) \\
  &= \sum_{i=1}^m y^{(i)} log h(x^{(i)}) + (1-y^{(i)}) log (1-h(x^{(i)}))
  \end{align}
  $$

- 参数优化（梯度上升）
  $$
  \begin{align}
  \frac{\partial L(\theta)}{\partial \theta_j} &= (y \frac{1}{g(\theta^Tx)} - (1-y) \frac{1}{1 -g(\theta^Tx)}) \frac{\delta g(\theta^Tx)}{\delta \theta_j} \\
  &= (y \frac{1}{g(\theta^Tx)} - (1-y)\frac{1}{1 -g(\theta^Tx)} ) g(\theta^Tx)(1-g(\theta^Tx)) \frac{\delta \theta^Tx}{\theta_j} \\
  &= (y (1 - g(\theta^Tx)) - (1-y) g(\theta^Tx)) x_j \\
  &= [y - h_{\theta} (x)]x_j \\
  \end{align}
  $$




$$
\begin{align}
\theta_j &= \theta_j + \alpha \frac{\partial L(\theta)}{\partial \theta} \\
&= \theta_j + \alpha [y^{(i)} - h_{\theta} (x^{(i)})]x_j^{(i)}   
\end{align}
$$



- 损失函数：
  $$
  J(\theta) = - \frac{1}{m}   \left[  \sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)}))   + (1-y^{(i)}) log(1 - h_\theta(x^{(i)}))             \right]
  $$




### 3. LR 如何实现多分类？

- **方式1：** 修改逻辑回归的损失函数，使用softmax函数构造模型解决多分类问题，softmax分类模型会有相同于类别数的输出，输出的值为对于样本属于各个类别的概率，最后对于样本进行预测的类型为概率值最高的那个类别。
- **方式2：** 根据每个类别都建立一个二分类器，本类别的样本标签定义为0，其它分类样本标签定义为1，则有多少个类别就构造多少个逻辑回归分类器。

若所有类别之间有明显的互斥则使用softmax分类器，若所有类别不互斥有交叉的情况则构造相应类别个数的逻辑回归分类器。

### 4. LR 为何要对特征进行离散化

- **非线性。** 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代； 
- **速度快。** 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展
- **鲁棒性。** 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
- **方便交叉与特征组合**： 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。
- **稳定性：** 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。
- **简化模型：** 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

### 5. 逻辑回归中，增大 L1 正则化会是什么结果

所有参数 w 都会变成 0。



