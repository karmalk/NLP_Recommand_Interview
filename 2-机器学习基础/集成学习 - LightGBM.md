# 集成学习 - LightGBM

---

## 1，简单介绍以下LightGBM

LightGBM 是一个实现GBDT算法的框架，支持高效率的**并行训练**,在Higgs数据集上LightGBM比XGBoost快将近10倍，内存占用率大约为XGBoost的1/6，并且准确率也有提升。GBDT在每次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复的进行读写数据又会 消耗非常大的时间，LightGBM提出的主要原因就是为了解决GBDT在海量数据遇到的问题，让GBDT更好更快的用于工业实践。

### 1.1，Light GBM在哪些地方进行了优化(区别XGBoost)

- 基于Histogram的决策树算法
- 带深度限制的Leaf-wise的叶子生长策略
- 直方图做差加速
- 支持类别特征
- Cache命中率优化
- 基于**直方图**的稀疏特征优化多线程优化。

### 1.2，简单说一下Histogram算法

直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数(其实就是分桶思想，这些桶成为bin),同时构造一个宽度为k的直方图

在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。

使用直方图算法有很多优点。首先就是**内存消耗的降低**，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降为原来的1/8,其次在计算上的代价也大幅度降低，预排序算法每遍历一个特征就需要计算一次分裂的增益，而直方图算法只需要计算k次，时间复杂度从O(data * feature)优化到O(k * feature)

### 1.3，带深度限制的Leaf-wise的叶子生长策略

XGBoost中，树是按层生长的(Level wise)，同一层的所有结点都做分裂，最后剪枝

Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但效率很低，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，实际上很多叶子的**分裂增益**较低，没必要进行搜素和分裂

Histogram中，LightGBM使用了带有深度限制的**按叶子生长**的leaf-wise的算法。

Leaf-wise是一种更为高效的策略，每次从当前所有叶子中，找到**分裂增益**最大的一个叶子，然后分裂。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。但缺点是可能会长出比较深二等决策树，从而**产生过拟合**，因此Light GBM在Leaf-wise之上增加了一个**最大深度的限制**，在保证高效率的同时防止过拟合。

### 1.4，直方图做差加速

Light GBM另一个优化是Histogram(直方图)做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲结点的直方图与它兄弟的直方图做差得。通常构造直方图，需要遍历叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。

### 1.4，直方图做差加速

大多数机器学习工具都无法直接支持类别型特征，一般需要把类别型特征转化到多维的0/1特征，降低了空间和时间的效率。而在实践中，类别型特征很常见，基于这个考虑，LightGBM优化了对类别型特征的支持，可以直接输入类别型特征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则，相比0/1展开的方法，训练速度可以加速8倍。

## 2，LightGBM的优点

- 更快的训练速度
- 更低的内存消耗
- 更高的准确率
- 分布式支持，可以快速处理海量数据。

