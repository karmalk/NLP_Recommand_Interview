# 集成学习 - GBDT

---

[TOC]

## 1. Boosting 思想

Boosting 基于串行策略， 各个基分类器之间有依赖。

1. 先从初始训练集训练一个弱学习器，初始训练集各个样本权重相同根据上一个弱学习器的表现，调整样本权重(对前一层基分类器分错的样本给予**更高**的权重)，使得分类错误的样本得到更多关注。
3. 基于调整后的样本分布，训练下一个弱学习器
4. 测试时，对各基学习器**加权**得到最终结果

Bagging和Boosting的串行训练方式不同，Bagging方法在训练的过程中，各基分类器之间无强依赖，可以进行并行训练。

## 2. GBDT

GBDT ， Gradient Boosting  Decision Tree， 叫 梯度提升决策树。使用的是Boosting的思想。

GBDT原理很简单，就是所有弱分类器的结果相加等于预测值，然后下一个弱分类器去拟合误差函数对预测值的残差(预测值和真实值之间的误差)，弱分类器的表现形式是各棵树。

### 2.1 为何GBDT可以用负梯度近似拟合残差？

回归任务下，GBDT在每一轮的迭代时对每个样本都会有一个预测值，此时损失函数为均方差损失函数$l(y_i,y^i) = \frac{1}{2}(y_i-y^i)^2$

那此时负梯度的计算公式为
$$
-\frac{\partial(y_i,y^i)}{\partial y^i} = (y_i-y^i)
$$
所以当损失函数选用均方损失函数时，每一次拟合的值就是(真实值-当前模型预测的值)，即残差，此时的变量是$y^i$(当前预测模型的值)，也就是对它求负梯度。

### 2.2 梯度提升和梯度下降的区别和联系是什么？

两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新，只不过在梯度下降中，模型是以参数化形式表示，从而模型的更新等价于参数的更新，而在梯度提升中，模型并不需要进行参数化表示，而是直接定义在函数空间中，从而大大扩展了可以使用的模型种类。

|   梯度提升   | 函数空间F     | $F=F_{t-1}-\rho _{t}\nabla_{F}L|_{F=F_{t-1}}$  | $L=\sum_{i}l(y_i,F(x_i))$   |
| :----------: | :------------ | :--------------------------------------------- | --------------------------- |
| **梯度下降** | **参数空间W** | $w_t = w_{t-1}-\rho_{t}\nabla_wL|_{w=w_{t}-1}$ | $L=\sum_{i}l(y_i,f_w(w_i))$ |

### 2.3，GBDT的优点和局限性由哪些？

**优点**

- 预测阶段的计算速度快，树与树之间可以并行化计算
- 在分布稠密的数据集上，**泛化能力**和**表达能力**都很好。
- 采用决策树作为弱分类器使得GBDT模型具有较好的**解释性**和**鲁棒性**，能够自动发现特征间的高阶关系，并且不需要对数据进行特殊的预处理(归一化)

**据线性**

- GBDT在**高维稀疏数据集**上，表现不如支持向量机或者神经网络
- GBDT在处理文本分类特征问题上，相对于其他模型的优势不如它在处理数值特征时明显。
- 训练过程需要**串行训练**，只能在决策树内部采用一些**局部并行**的手段提高训练速度。

## 3 RF 与GBDT之间的区别和联系

**相同点**

都是由多棵树组成，最终的结果都是由多棵树一起决定，都是集成学习算法

**不同点**

- 组成随机森林的树可以是**分类树**，也可以是**回归树**，但GBDT只由回归树组成
- 组成随机森林的树可以并行生成,GBDT只能串行生成
- 随机森林的结果是多数表决(基于bagging算法),而GBDT则是多棵树累加之和(Boosting算法)
- 随机森林对异常值不敏感，而GBDT对异常值比较敏感
- 随机森林是减少模型的方差，而GBDT是减少模型的偏差
- 随机森林不需要进行特征归一化，GBDT需要进行特征归一化。



