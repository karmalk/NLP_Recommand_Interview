# 面试套路

tags: NLP

---

## 1. 自我介绍

您好，我是北京科技大学的计算机研究生，本科毕业于北京化工大学。 本科期间做过一段时间的Android 和嵌入式。 研究生期间主要研究方向是自然语言处理中的阅读理解。 

- 研一上半学期主要是学习机器学习，深度学习方面的基础， 研一下主要是阅读自然语言方面的论文和确定自己的研究方向，4月中旬决定要做**高考阅读理解**这个方向，开始阅读阅读理解领域的相关论文。后来，考虑到数据方面的因素，暑假去爱奇艺实习了一段时间，主要做爬虫和数据方面的工作。 
- 研二上学期主要是做阅读理解相关的实验与跟进最新的论文，前期主要探索能不能参考 SQUAD 数据集上的模型对 RACE 数据集上现有的 Attention 机制进行改进。 后来预训练语言模型爆发，直接拉高 benchmark， 因此研二下主要是看看，能不能在 Bert 之上(如 Attention）做一些文章。后来，觉得自己方向做的太窄了， 因此，慢慢扩展到文本分类，文本相似度这些方向。

此外，我知乎专栏写的还不错， 可以看看。 我自己也很喜欢跟别人分享知识，我在搜狗实习，每周五都会分享一些最新的论文和技术。

我觉得我现在是处于 **论文量足够大**， **读论文也很快**（差不多3个小时精读一篇），也能很快实现， 看论文也能产生一些idea， 但大多数创新点都不够顶会的级别， 没有资源和时间去做了， 毕竟马上就要毕业了。

## 2. 看了这么多论文，有什么idea吗？（快手第一问）

### 1. 简单idea -- 文本分类（在做ing）

- 文本长度对 Bert 在文本分类任务上微调的影响
- 如何将长文本信息融合
- 探索在 Bert 之上做文本分类的效果， 比如将经典的一些网络融进去，看看有没有提升，并分析对不同的数据集为何会有这种提升。

### 2. 复杂一点的idea -- 阅读理解

RACE 数据集本身其文本长度在 2000 左右，而 Bert 的限制在 512， 因此，如何融入这种长文本信息。 

- 加 x 层 Transformer： 先分别对段落进行 embedding， 然后采用 Transformer 进行信息融合 -- 效果有所提升
- 设计精巧的 Attention 机制： 比如article 与 question 之间的匹配方式等， 目前正在尝试。

### 3. 多维向量融合 -- 多种数据集

对于高维向量， 预训练语言模型向量的融合， 加一层 cnn， lstm， transformer 向量的融合。

### 4. 探索 Attention 在语义匹配机制中的选择



### 3. 关于预训练语言模型 + 知识图谱  - 雏形（做不了，没资源）

idea 来源于 百度的 ERNIE 与 清华的 ERNIE ， 这两篇文章采用了不同的方式融合知识图谱信息。 

- 百度的方式能否融入 实体与实体之间的关系？ 如何验证？
- 如果不能， 那么如何更好的融入 实体与实体之间 的关系。（思路：设计一个类似 NP 的预训练网络，跟语言模型一起训）

百度的文章对比清华的文章，虽然创新度有所不足，但更看好百度的方向，毕竟大道至简， 清华的那篇文章着实复杂了些， 太多中间过程可能反而引入了很多噪声。

###  4. 预训练语言模型 + 自然语言生成 -- 下一个爆点（雏形）

这点还需要看生成方面的诸多论文。

### 5. 信息融合 - 受到 char + word embdding 启发、



## 3. 你有什么想问的吗

- 请问在贵部门，算法工程师主要负责哪些事情？
- 您怎么看目前预训练语言模型在NLP中的发展？
- 您觉得算法工程师的核心技能是什么？

## 4. HR 的提问

- 您觉得， 我需要加强哪些方面的知识？
- 你对我接下来的学习有什么建议吗？
- 能谈谈对我此次表现的评价吗？

## 5. 部门领导的提问

- 您怎么看到预训练语言模型，它会一统江湖吗？
- 您觉得，我在刚开始工作1-3年内，如何提升专业技术？





