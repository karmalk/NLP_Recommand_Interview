# 激活函数

tags: 深度学习

---

[TOC]

## 前言

本文先对激活函数的特性，常见的激活函数以及如何选择合适的激活函数。

**需要注意的是，激活函数是来向神经网络中引入非线性因素的，通过激活函数，神经网络就可以拟合各种曲线。**可参考：[激活函数，你真的懂了吗？](https://zhuanlan.zhihu.com/p/44398148)

## 1. 激活函数的性质

- **非线性：**为模型引入非线性因素

- **几乎处处可微：**有限的不可微点有左右导数（左右导数可能不同，如Relu）。 便于反向传播，利于优化

- **计算简单：**激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。

- **非饱和性：**饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。

- **单调性：**当激活函数是单调的时候，单层网络能够保证是凸函数；

- $ f(x)≈x $： 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值。 

  由于这个条件与非线性有点矛盾，因此激活函数基本只是部分满足这个条件，如 relu 只再 x>0 时为线性。

- **输出值的范围有限：** 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。

  当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。

- **参数少：** 大部分激活函数都是没有参数的。

- **归一化：** 主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。

## 2. 激活函数一览 -- TODO

#### 1. sigmoid



#### 2. tanh

tanh 本质上是 sigmoid 向下平移和伸缩后的结果。



#### 3. Relu





## 3. 如何选择激活函数

- 如果是二分类问题， 输出层是sigmoid，其余层是Relu
- 一般隐层采用Relu， 有时也要试试 tanh， 这两大函数的变体都有必要试试

## Relu 的优点

- Relu 不耗费资源，且导数为1， 学习起来较快
- sigmoid， tanh 的导数在正负饱和区的梯度都会接近于0， 这会造成梯度消失。
- Relu 有Dead Relu 问题，此时试试其变体， 如Leaky Relu

## 激活函数的稀疏激活性

从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 $ x<0 $ 时，ReLU 硬饱和，而当 $ x>0 $ 时，则不存在饱和问题。ReLU 能够在 $ x>0 $ 时保持梯度不衰减，从而缓解梯度消失问题。



---

## QA

## 0. Dead Relu 问题

某些神经元可能永远不会被激活， 导致其相应的参数永远不能被更新。其本质是**由于Relu在的小于0时其梯度为0所导致的。**

首先我们假设Relu的输入是一个低方差中心在+0.1的正态分布， 此时假设现在大多数Relu的输入是正数，那么大多数输入经过Relu函数能得到一个正值， 因此此时大多数输入能够反向传播通过Relu得到一个梯度， 于是我们的Relu的输入就完成了更新。

假设在随机反向传播中， 有一个巨大的梯度经过了Relu且此时Relu的输入为正（Relu是打开的）， 那么该梯度会引起Relu输入X的巨大变化， 假设此时输入X的分布变成了一个中心在-0.1 的正态分布。此时的情况如下：

> 首先， 大多数Relu的输入变为负数， 输入经过Relu函数就能得到一个0， 这也意味着大多数输入不能反向传播通过Relu得到一个梯度，导致这部分输入无法通过更新。

### 1. Relu VS Sigmoid VS tanh

- sigmoid 缺陷：

  > - 极容易导致梯度消失问题
  > - 计算费时
  > - **sigmoid 函数不是关于原点中心对称的**

- tanh 缺陷： 无法解决梯度消失问题

- Relu 优点：

  > - **一定程度上缓解了梯度问题：** 其导数始终为一个常数
  > - **计算速度非常快：** 求导不涉及浮点运算，所以速度更快
  > - **减缓过拟合：** `ReLU` 在负半区的输出为 0。一旦神经元的激活值进入负半区，那么该激活值就不会产生梯度/不会被训练，造成了网络的稀疏性——**稀疏激活**， 这有助于减少参数的相互依赖，缓解过拟合问题的发生

### 2. 为什么Relu 不是全程可微也能用于基于梯度的学习？

虽然 ReLU 在 0 点不可导，但是它依然存在**左导数和右导数**，只是它们不相等（相等的话就可导了），于是在实现时通常会返回左导数或右导数的其中一个，而不是报告一个导数不存在的错误。

### 3. 为何加入非线性因素能够加强网络的表示能力？

- 神经网络的万能近似定理：神经网络只要具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何**从一个有限维空间到另一个有限维空间**的函数。
- 如果不使用非线性激活函数，那么每一层输出都是上层输入的**线性组合**；此时无论网络有多少层，其整体也将是线性的，这会导致失去万能近似的性质
- 但仅**部分层是纯线性**是可以接受的，这有助于**减少网络中的参数**。

### 4. 为何 tanh 比 sigmoid 收敛快？

$$
tanh^{'}(x)=1-tanh(x)^{2}\in (0,1) \\
sigmoid^{'}(x)=sigmoid(x)*(1-sigmoid(x))\in (0,\frac{1}{4}]
$$

