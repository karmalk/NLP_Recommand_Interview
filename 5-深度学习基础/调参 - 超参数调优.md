# 超参数调优

tags: 深度学习

---

[TOC]

<https://zhuanlan.zhihu.com/p/29247151>

## 1. 超参数一览

### 1. 超参数是什么？

- 参数： 需要训练，指的是模型训练中的权重参数和 bias 参数。
- 超参数： 不需要训练，需要在训练前进行指定，并不断调整。

其实就很多超参数来说，调整的意义并不大，毕竟往往网络的超参数多达几十个，要是都精调的话，那岂不是得 gg， 因此往往是对重要参数精调，对次要参数粗调。

此外，很多 Trick 往往需要一些其他的超参数，对于这部分参数，往往我会遵循原论文，适当的调一调就行。毕竟，Trick 无穷尽呀。

### 2. 网络结构参数

网络参数指的是你自己构建网络结构时的相关参数，如卷积核数量，网络层数等

- CNN 网络参数

  | 超参数      | 说明          | 推荐值                                         |
  | ----------- | ------------- | ---------------------------------------------- |
  | kernel size | 卷积核的 size | 一般为奇数：[7 * 7], [5 * 5], [3 * 3], [1 * 1] |
  | kernel num  | 卷积核的数量  | 一般在 [100, 600] 间探索                       |
  |             |               |                                                |

- RNN 网络参数

  | 超参数 | 说明 | 推荐值 |
  | ------ | ---- | ------ |
  |        |      |        |
  |        |      |        |
  |        |      |        |

- Transformer 参数

  | 超参数 | 说明 | 推荐值 |
  | ------ | ---- | ------ |
  |        |      |        |
  |        |      |        |
  |        |      |        |

### 2. 优化参数

优化参数指的是反向传播中所涉及到的参数，主要包括：学习率， batch_size， 对应优化器参数， 损失函数参数等。

- 常见参数

| 超参数        | 说明                           | 推荐值     |
| ------------- | ------------------------------ | ---------- |
| learning rate | 最重要的参数，需要精调         | 下文有推荐 |
| batch size    | 次要重要参数，需要精调         | [1: 1024]  |
| dropout       | 解决过拟合的重要参数，需要精调 | [0: 0.5]   |

- 优化器相关参数：

  | 优化器 | 参数说明 | 推荐值 |
  | ------ | -------- | ------ |
  | Adam   |          |        |
  |        |          |        |
  |        |          |        |

- 正则化参数：

  | 超参            | 说明 | 推荐值    |
  | --------------- | ---- | --------- |
  | L2 权重衰减系数 |      | [0, 1e-4] |
  |                 |      |           |
  |                 |      |           |


### 3. Trick 参数

| 超参数 | 说明 | 推荐值 |
| ------ | ---- | ------ |
|        |      |        |
|        |      |        |
|        |      |        |

## 2. 几个重要的超参数

### 1. 学习率 -- 最重要的超参数

学习率直接控制着梯度更新时的量级，从而直接影响模型的优化与最终的有效容量。 幸运的是，对于学习率的设置，已经有一套行之可效的指导方案了， 针对不同的优化器，有不同的设置区间。 

如果是微调，那么学习率要降低两个数量级左右（参考 Bert 的 adam 学习率)

| 优化器   | 设置范围     |
| -------- | ------------ |
| SGD      | [1e-2 ,1e-1] |
| Momentum | [1e-3, 1e-2] |
| Adagrad  | [1e-3, 1e-2] |
| Adadelta | [1e-2, 1e-1] |
| RMSprop  | [1e-3, 1e-2] |
| Adam     | [1e-3, 1e-2] |
| Nadam    | [1e-3, 1e-2] |

### 2. batch size

一般情况下，  batch size 我往往会以 128 为起点，上下调整，注意，batch size 要设置为 2 的幂次方， 范围在 [1, 1024] 之间。

此外，需要一提的是 Batch Normalization 与 batch size 息息相关，如果你使用了 Batch Normalization， 那么 batch size 就不能设的太小， 这点我在 Normalization 那一节中有详细解释。

### 3. dropout

dropout 我往往会设置先为 0.5， 然后在 [0.0, 0.5] 范围内精调。

Dropout 往往会在卷积层和全连接层之间是有来防止过拟合。 使用 Dropout 需要注意两点：

- 在RNN中，如果直接放在memory cell中,循环会放大噪声，扰乱学习。一般会建议放在输入和输出层；
- 不建议dropout后直接跟上batchnorm，dropout很可能影响batchnorm计算统计量，导致方差偏移，这种情况下会使得推理阶段出现模型完全垮掉的极端情况；

### 4. 优化器参数

对于优化器的一些参数，我往往会采取默认值，这是因为，默认值都是论文最初的设置，一般都能够获得不错的表现，我个人一般不做很精细的调试，也不建议这样去做。

## 超参数调优策略

**采用2的幂次方作为 batch_size 的值，并在对数尺度上对学习率进行采样。**

### 1. 网格搜索

- 定义一个 n 维的网格，每一格都有一个超参数。
- 对于每个维度，定义可能的取值范围
- 搜索所有可能的配置并获得最佳结果

我个人一般还是用 Markdown 表格来做记录， 如下：

| 优化算法 | 学习率 | batch_size |
| -------- | ------ | ---------- |
| adam     | 1e-5   | 128        |

- **缺点：**该方法痛点真的很痛，那就是：**维数灾难**。 随着要精调的超参数的增加，搜索在时间复杂度上也会增加的越多（指数级别），最终使得该策略不可行。
- **优点：** 如果采用较大的搜索范围以及较小步长，该方法有很大概率能找到全局最优值

因此， 我一般尽可能少的去调节次要超参数，比如优化算法默认 Adam 等。此外， 先进行粗调来寻找全局最优值可能的位置，然后采用精调的策略寻找更精确的最优值。

一般只有超参数在 4 个以内才使用网格搜索，不然太费时间了。

### 2. 随机搜索

随机搜索在搜索范围内随机选取样本点，它认为如果样本点集足够大，那么通过随机采样也能大概率的找到全局最优值或其近似值。

- 优点： 比网格搜索要快
- 缺点：结果无法保证，很依靠调参经验。

我一般都是以推荐超参数设置方案来作为第一次的设置，然后围绕这个设置点上下浮动。

### 3. 贝叶斯优化

网格搜索与随机搜索都是独立于之前的训练的，

https://zhuanlan.zhihu.com/p/29779000

贝叶斯则是利用历史的搜索结果进行优化搜索。其主要有四部分组成，

- 目标函数，大部分情况下就是模型验证集上的损失。
- 搜索空间，即各类待搜索的超参数。
- 优化策略，建立的概率模型和选择超参数的方式。
- 历史的搜索结果。

首先对搜索空间进行一个先验性的假设猜想，即假设一种选择超参的方式，然后不断的优化更新概率模型，最终的目标是找到验证集上误差最小的一组超参数。

---

## QA

### 1. 为何学习率那么重要？

当模型训练到一定程度后， 损失将不再减少，这个时候模型的一阶梯度接近于0，此时的Hessian 矩阵通常是两种情况：

- 正定，即所有特征值均为正，此时通常可以得到一个局部极小值，若这个局部极小值接近全局最小则模型已经能得到不错  的性能了，但若差距很大，则模型性能还有待于提升，通常情况下后者在训练初最常见。
- 特征值有正有负，此时模型很可能陷入了鞍点，若陷入鞍点，模型性能表现就很差。

### 2. 卷积核尺寸为何都是奇数？

- 保证像素点中心位置，避免位置信息偏移
- 填充边缘时能保证两边都能填充，原矩阵依然对称

### 3. 深层网络为何难以训练

- 梯度消失， 梯度爆炸问题

### 4. 神经网络为何要做深？

- 神经元数量相同的情况下，深层网络比浅层网络具有更大容量和表达空间。
- 隐藏层增加意味着由激活函数带来的非线性变换的嵌套层数更多，就能构造更复杂的映射关系。

### 5. 调节 batch_size 对训练效果影响如何？

- Batch_size 太小，模型表现效果极其糟糕(error )



### 6. 合理增加 batch size 有何好处？

- 内存
- 利用率提高了，大矩阵乘法的并行化效率提高。
- 跑完一次 epoch 所需的迭代次数减少，对于相同数据量的处理速度进一步加快。
- 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。

### 7. 盲目增大 Batch_Size 有何坏处？

- 内存，显存容量可能撑不住
- 跑完一次 epoch 所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
- Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。