# 局部最小值，鞍点，退化

---

## 基础原理

**局部最小值**

神经网络的损失函数一般不是凸函数，因此，在训练时有陷入到局部最小值的风险，梯度下降法只能保证收敛到梯度为0的点，而这不一定是局部极值点，更不能保证是全局最优值点

**鞍点**：鞍点是指梯度为0但Hessian矩阵不定的点，不等于局部极值点。




[神经网络最终收敛何处？](<https://zhuanlan.zhihu.com/p/48737640>)



## QA

---

### 1. 如何避免陷入局部最小值与鞍点？

- **SGD 或 Mini-batch**：SGD 与 Mini-batch 引入了随机性，每次以部分样本来计算梯度，能够相当程度上避免陷入局部最小值。
- **动量**： 引入动量，相当于引入惯性。一些常见情况时，如上次梯度过大，导致进入局部最小点时，下一次更新能很容易借助上次的大梯度跳出局部最小点。
  - 在学习率较小的时候，适当的Momentum(动量)能够起到一个加速收敛的过程
  - 在学习率较大的时候，适当的Momentum(动量)能够起到一个减小收敛时震荡幅度的作用。
- **自适应学习率**：通过学习率来控制梯度是一个很棒的思想， 自适应学习率算法能够基于历史的累计梯度去计算一个当前较优的学习率。

