# 特征工程

tags: 机器学习

---

[TOC]

![8.数据清洗与特征处理](..\img\8.数据清洗与特征处理.jpg)

## 1. 如何处理数据中的缺失值

有些特征可能因为无法采样或者没有观测值而缺失，此时需要对这些缺失的特征值进行特殊处理。

### 1. 缺失值较多

如果该特征中的缺失值较多，则应该直接舍弃，否则反而可能引入较大噪声，造成反效果。

### 2. 缺失值较少

即当缺失值在 10% 以内时，我们可以采用多种方式处理：

- 将缺失值当一个特征处理，用 一个异常值表示， 如0

  ```
  data_train.fillna(0) 
  ```

- 用均值填充

  通常一个策略是取相同 label 的数据的均值

  ```
  data_train.fillna(data_train.mean()) 
  ```

- 以上下数据填充

  ```
  data_train.fillna(method='pad')  # 上一个数据填充
  data_train.fillna(method='bfill')  # 下一个数据填充
  ```

- 插值法

  ```
  data_train.interpolate() # 即估计中间点的值
  ```

- 用随机森林等算法拟合

  > 将数据分为有值和缺失值2份，对有值的数据采用随机森林拟合，然后对有缺失值的数据进行预测，用预测的值来填充。

## 2.  特征常见处理手段

### 1.数值归一化

归一化的目的是将所有的特征都统一到一个大致相同的数值区间中。

从梯度下降的角度来看，对于两个特征x1，x2， x1的范围远远大于x2，在学习率相同的情况下，x1的更新速度会大于x2，需要较多的迭代才能得到最优解。

![](http://ww1.sinaimg.cn/large/006gOeiSly1g0r7bxl5kfj30ie0a840y.jpg)

- 函数归一化： min-max 归一化， 零均值归一化（参考 **Normalization** 一节）
- 分维度归一化
- 排序归一化

### 2. 离散化

连续值的离散化：

- **等值划分：**将特征按照值域进行均分，每一段内的取值等同处理。例如某个特征的取值范围为[0，10]，我们可以将其划分为10段，[0，1)，[1，2)，...，[9，10)。
- 等量划分是根据样本总数进行均分，每段等量个样本划分为1段。

区别：例如距离特征，取值范围［0，3000000］，现在需要切分成10段，如果按照等比例划分的话，会发现绝大部分样本都在第1段中。使用等量划分就会避免这种问题，最终可能的切分是[0，100)，[100，300)，[300，500)，..，[10000，3000000]，前面的区间划分比较密，后面的比较稀疏。

## 3. 共线性问题  -- TODO

对于回归算法而言， 其首先假设回归模型的解释变量之间不存在线性关系， 即解释变量X1，X2，……，Xk中的任何一个都不能是其他解释变量的线性组合。如果违背这一假定，即线性回归模型中某一个解释变量与其他解释变量间存在线性关系，就称线性回归模型中存在多重共线性。

多重共线性违背了解释变量间不相关的古典假设，将给普通最小二乘法带来严重后果。

其实，简单来说，就是特征冗余，容易导致过拟合。

### 1. 如何判定共线性问题？

- 相关性分析。当相关性系数高于0.8，表明存在多重共线性；但相关系数低，并不能表示不存在多重共线性
- 方差膨胀因子VIF。当VIF大于5或10时，代表模型存在严重的共线性问题；
- 条件系数检验。 当条件数大于100、1000时，代表模型存在严重的共线性问题。

### 2. 如何消除共线性问题？

通常可通过PCA降维、逐步回归法和LASSO回归等方法消除共线性。



## 4. 如何进行特征选择？

### 1. 特征分类

- 相关特征： 对于特定的任务和场景有一定帮助的属性，这些属性能有效提升算法性能。
- 无关特征：在特定的任务和场景下完全无用的属性，这些属性对对象在本目标环境下完全无用。
- 冗余特征：同样是在特定的任务和场景下具有一定帮助的属性，但这类属性已过多的存在，不具有产生任何新的信息的能力。

### 2. 如何考虑特征选择

可以从以下两个方面来选择特征：

- 特征是否具有发散性：某个特征若在所有样本上的都是一样的或者接近一致，即方差非常小。 也就是说所有样本的都具有一致的表现，那这些就不具有任何信息。
- 特征与目标的相关性：与目标相关性高的特征，应当优选选择。

### 3. 特征选择方法分类

- 过滤法：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。  
- 包装法：根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。  
- 嵌入法：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。

### 4. 特征选择目的

- 减少特征维度，使模型泛化能力更强，减少过拟合;  
- 降低任务目标的学习难度；
- 一组优秀的特征通常能有效的降低模型复杂度，提升模型效率 

## 5. 关联规则

https://www.jianshu.com/p/7d459ace31ab

### 1. 简介

关联规则挖掘是一种基于规则的机器学习算法，该算法可以在大数据库中发现感兴趣的关系。它的目的是利用一些度量指标来分辨数据库中存在的强规则。也即是说关联规则挖掘是用于知识发现，而非预测，所以是属于**无监督**的机器学习方法。

从所有可能规则的集合中选择感兴趣的规则需要利用一些度量方法来筛选和过滤，下面的三种方法。

### 关联规则的三个度

#### 1. 支持度

$$
Support(X→Y) = P(X,Y) / P(I) = P(X∪Y) / P(I) = num(XUY) / num(I)
$$

支持度表示 item-set {X,Y} 在总 item-set 里出现的概率。其中， $I$ 表示总事务集， num()表示事务集里特定 item-set 出现的次数。比如：$num(I)$ 表示总事务集的个数，$num(X∪Y)$ 表示含有 {X,Y} 的事务集的个数（个数也叫次数）。

#### 2. 置信度

$$
Confidence(X→Y) = P(Y|X)  = P(X,Y) / P(X) = P(XUY) / P(X)
$$

置信度表示在先决条件X发生的情况下，由关联规则”X→Y“推出Y的概率。即在含有X的项集中，含有Y的可能性。

#### 3. 提升度

$$
Lift(X→Y) = P(Y|X) / P(Y)
$$

提升度表示含有X的条件下，同时含有Y的概率，与Y总体发生的概率之比。

满足最小支持度和最小置信度的规则，叫做“强关联规则”。

- $Lift(X→Y)>1$，“X→Y”是有效的强关联规则。
- $Lift(X→Y) <=1$，“X→Y”是无效的强关联规则。
- 特别地，$Lift(X→Y) =1$，X与Y相互独立。

------

## QA

### 1. 判断规则的有效性

题目：已知有1000名顾客买年货，分为甲乙两组，每组各500人，其中甲组有500人买了茶叶，同时又有450人买了咖啡；乙组有450人买了咖啡，如表所示，**题目：茶叶→咖啡是一条有效的关联规则吗？**

| 组次        | 买茶叶的人数 | 买咖啡的人数 |      |
| ----------- | ------------ | ------------ | ---- |
| 甲组(500人) | 500          | 450          |      |
| 已组(500人) | 0            | 450          |      |

**解答：**

- 茶叶--> 咖啡的支持度为： $Support(X-->Y)  = 450/500 = 90\%$
- 茶叶-->咖啡的置信度为： $Confidence(X-->Y) = 450/500 = 90\%$
- 茶叶-->咖啡的提升度为：$Lift(X-->Y) = Confidence(X-->Y) / P(Y) = 90\% / ((450+450) / 1000) = 1$

由于提升度Lift(X→Y) =1，表示X与Y相互独立，即是否有X，对于Y的出现无影响。也就是说，是否购买咖啡，与有没有购买茶叶无关联。即规则”茶叶→咖啡“不成立，或者说关联性很小，几乎没有，虽然它的支持度和置信度都高达90%，但它不是一条有效的关联规则。

### 2. 如果数据有问题，怎么处理?

- 上下采样平衡正负样例比
- 考虑缺失值
- 数据归一化