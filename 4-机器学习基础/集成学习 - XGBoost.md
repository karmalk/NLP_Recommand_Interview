# 集成学习 - XGBoost

## 1 简单介绍一下XGBoost

首先需要说一说GBDT，它是一种基于boosting**增强策略的加法模型**，训练的时候采用**前向分布算法**进行**贪婪学习**，每次迭代都学习一棵CART树来拟合之前t-1棵树的预测结果与训练样本真实值的残差。

XGBoost对GBDT进行了一系列的优化，比如损失函数进行了**二阶泰勒展开**，目标函数加入了**正则项**，**支持并行**和**默认缺失值处理**等等，在可扩展性和训练速度上有了很大的提升，但其核心思想并没有大的变化。

## 1-2 简单介绍一下GBDT

提到GBDT之前，一定要说一下Boosting，Boosting是一种与Bagging很类似的技术。无论是Boosting还是Bagging，所使用的多个分类器类型都是一致的。但是在Boosting中，不同的分类器是通过**串行训练**而获得的，每个新分类器都根据已训练的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。

由于Boosting分类的结果是基于所有分类器的**加权求和**结果的，因此Boosting与Bagging不太一样，Bagging中的分类器权值是一样的，而Boosting中的分类器权重并不相等，每个权重代表对应分类器在上一轮迭代的成功度。

GBDT与传统的Boosting区别还有点大，**它的每一次计算都是为了减少上一次的残差**，而为了消除残差，我们可以在残差减小的梯度方向上建立模型,所以说，在GradientBoost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的Boosting中关注正确错误的样本加权有很大的区别。

在GradientBoosting算法中，关键就是**利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树**

GBDT会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树。

## 2 XGBoost和GBDT有什么区别

- **基分类器**：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的逻辑回归(分类问题)或者线性回归(回归问题)
- **导数信息**:XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息。并且在损失函数一阶，二阶可导的条件下，XGBoost可以自定义损失函数。
- **正则项**：XGBoost的目标函数加了正则项，相当于预剪枝，使得学习出来的模型更加不容易过拟合。
- **列抽样**：XGBoost支持列抽样，与随机森林类似，用于防止过拟合。
- **缺失值处理**：对树中的每个非叶子节点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值确缺失，会将其划入默认分支。
- **并行化**：XGBoost 的并行化并不是指的tree维度的并行，而是特征维度的并行，XGBoost预先将每个特征按特征值排好序，存储为块(block)结构，分裂节点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。这个块存储结构也使得并行成为可能，在进行节点分裂时，需要计算每个特征的信息增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程并行。

## 2-2 XGBoost和Light GBM的区别

- 1 **树增长策略**：XGB采用**level-wise**的分裂策略，LGB采用**leaf-wise**的分裂策略。XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制。
- 2 **分割点查找算法**：*XGB使用**特征预排序**算法*，*LGB使用基于直方图的切分点算法*，其优势如下：
  - **减少内存占用**，比如离散为256个bin时，只需要用8位整型就可以保存为一个样本被映射位哪个bin(这个bin可以说就是转换后的特征)，对比预排序的exact greedy算法来说，(用int_32来存储索引+用float_32保存特征值)，可以节省7/8的空间
  - **计算效率提高**。预排序的Exact greedy对每个特征都需要遍历一边数据，并计算增益，复杂度位O(#feature x #data)。而直方图算法在建立王直方图后，只需要对每个特征遍历直方图即可，复杂度为O(#feature x #bin)
  - **LGB 还可以使用直方图做差加速**，一个节点的直方图可以通过**父节点的直方图**减去**兄弟节点的直方图**得到，从而加速计算。
    - 实际上XGBoost中共的近似直方图也类似与light GBM中的直方图算法，但是xgb的近似算法比lgb慢很多主要是因为xgboost在每一层都动态构建直方图，因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature都共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。
- **3 支持离散变量**：xgb无法直接输入类别型变量，因此需要实现对类别型变量进行编码(如独热编码)，而lightgbm可以直接处理类别型变量。
- **4 缓存命中率**：XGB使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。
- **5 LightGBM和XGBoost的并行策略不同**：
  - **特征并行**： LGB特征并行的前提是每个worker留有一份完整的数据集，但是每个worker仅在特征子集上进行最佳切分点的寻找；worker之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个worker进行切分即可。XGB的特征并行与LGB的最大不同在于XGB每个worker节点中仅有部分的列数据，也就是垂直切分，每个worker寻找局部最佳切分点，worker之间相互通信，然后在具有最佳切分点的worker上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他worker才能开始分裂。二者的区别就导致了LGB中worker间通信成本明显降低，只需通信一个特征分裂点即可，而XGB中要广播样本索引。 
  - **数据并行**： 当数据量很大，特征相对较少时，可采用数据并行策略。LGB中先对数据水平切分，每个worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得worker间的通信成本降低一倍，因为只用通信以此样本量少的节点。XGB中的数据并行也是水平切分，然后单个worker建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个worker上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个worker间的通信量也就变得很大。 
  - **投票并行(LGB)**：当数据量都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其当特征维度很大时，大致思想是：每个worker首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。

## 3 XGBoost为什么用二阶泰勒展开

- **精准性**：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数。
- **可扩展性**：损失函数支持自定义，只需要新的损失函数二阶可导

## 4 XGBoost为什么可以并行训练

- XGBoost的并行，并非每棵树可以并行训练，XGB本质上还是采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。
- XGBoost的并行，指的使**特征维度**上的并行，在训练之前，每个特征按照特征值对样本进行预排序，并存储为**block结构**，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分裂点时，可以利用多线程对每个block并行计算。

## 5 XGBoost为什么快

- **分块并行**：训练前每个特征按照特征值排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点。
- **候选分位点**：每个特征采用常数个分位点作为候选分割点。
- **CPU cache 命中优化**：使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中。
- **Block处理优化**：Block预先放入内存，Block按列进行解压缩，将Block划分到不同硬盘来提高吞吐。

## 6 XGBoost防止过拟合的方法

XGBoost在设计时，为了防止过拟合做了很多优化。具体如下：

- **目标函数添加正则项**：叶子节点个数+叶子节点权重的L2正则化
- **列抽样**：训练的时候只用一部分特征(不考虑剩余的Block块即可)
- **子采样**：每轮计算可以不适用全部样本，使得算法更加保守
- **shrinkage**：学习率/步长,为了给后面的训练留出更多的学习空间。

## 7 XGBoost如何处理缺失值

- 在特征K上寻找最佳split point时，不会对该列特征missing的样本进行遍历，而只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找split point的时间开销
- 在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向(左分支或者右分支)，作为预测时特征值缺失样本的默认分支方向
- 如果在训练中没有缺失值而在预测中出现缺失，那么会自定将缺失值的划分方向放到**右子结点**

## 8 XGBoost中叶子结点的权重如何计算出来

XGBoost的目标函数最终推到形式如下:
$$
Obj^{(t)} = \sum_{j=1}^{T}[G_{j}w_{j} + \frac{1}{2}(H_{j}+\lambda)w_{j}^{2}] + \gamma T
$$
利用一元二次函数求最值的知识，当目标函数达到最小值Obj*时，每个叶子结点的权重为wj*

具体公式如下
$$
w^{*}_{j} = - \frac{G_{j}}{H_{j}+\lambda}  每个叶子结点的权重
$$

$$
Obj = -\frac{1}{2}\sum_{j=1}{T}\frac{G_{j}^{2}}{H_{j}+\lambda} +\gamma T	第t颗树带来的最小损失(训练损失+正则损失)
$$

## 9 XGBoost中的一颗树停止生长条件

- 当新引入一次分裂所带来的**增益Gain<0**时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程
- 当树**达到最大深度**时，停止建树，因为树的深度太深容易出现过拟合现象，这里需要设置一个超参数，max_depth
- 当引入一次分裂后，重新计算新生成的左右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数：**最小样本权重和**，是指如果一个叶子结点包含的样本数量太少也会放弃分裂，防止树分的太细。

## 10 RF 和GBDT的区别

**相同点**

- 都是由多颗树组成，最终的结果都是由多棵树一起决定

**不同点**

- **集成学习**：RF属于bagging思想，而GBDT属于boosting思想
- **偏差-方差权衡**：RF不断的降低模型的**方差**，而GBDT不断的降低模型的**偏差**
- **训练样本**：RF每次迭代的样本都是从全部训练集中由放回抽样形成的，而GBDT每次使用全部样本
- **并行性**：RF树可以并行生成，而GBDT只能串行生成（需要等上一颗树完全生成）
- **最终结果**：RF最终是多棵树进行多数投票表决(回归问题是取平均)，而GBDT是加权融合
- **数据敏感性**：RF对异常值不敏感，而GBDT对异常值比较敏感
- **泛化能力**：RF不易过拟合，而GBDT容易过拟合

## 11 XGBoost如何处理不平衡数据

- 在意AUC，采用AUC评估模型的性能，可以通过scale_pos_weight来平衡正样本和负样本的权重
- 在意概率(预测得分的合理性),不能重新平衡数据集(会破坏数据的真实分布),应该设置max_delta_step为一个有限数字来帮助收敛(基模型为LR时有效)
- 通过上采样，下采样，SMOTE算法或者自定义损失函数的方式解决正负样本不平衡的问题。

## 12 比较LR和GBDT。什么情景下GBDT不如LR

区别

- LR时线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程。
- GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树和树之间无法并行训练，而且树模型很容易过拟合。

当再**高维稀疏**特征的场景下，LR的效果一般会比GBDT好。

-  现在的模型普遍都带有正则项，而LR等线性模型的正则项是对权重的惩罚，也就是W1一旦过大，惩罚就会很大，进一步压缩W1的值，使他不至于过大。但是树模型则不一样，树模型的惩罚项通常为**叶子节点数**和**深度**等。

**在高维稀疏特征的时候，线性模型会比非线性模型效果好的原因：带正则化的线性模型比较不容易对稀疏特征过拟合**

## 13 XGBoost中如何对树进行剪枝

- 在目标函数中增加了**正则项**：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度
- 在结点分裂时，定义了一个**阈值**，如果分裂后目标函数的增益小于该阈值，则不分裂。
- 当引入一次分裂后，重新计算新生成的左右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值(最小样本权重和),也会放弃此次分裂。
- XGBoost先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。

## 14 XGBoost怎么选择最佳分裂点

XGBoost在训练前预先将特征按照特征值进行排序，并存储为Block结构，以后再结点分裂时可以重复使用该结构。

因此，可以采用**特征并行**的方法利用多个线程分别计算每个特征的最佳分裂点，根据每次分裂后产生的增益，最终选择增益最大的那个特征的特征值作为最佳分裂点。

如果再计算每个特征的最佳分割点时，对每个样本都进行遍历，计算复杂度会很大，这种全局扫描的方法并不适用大数据场景下。XGBoost提供了一种直方图近似算法，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。

## 15 XGBoost的Scalable性如何题想

- 基分类器：弱分类器可以支持CART决策树，也可以支持LR和Linear
- 目标函数：支持自定义loss　function　只需要一阶和二阶可导，有这个特性是因为二阶泰勒展开，得到通用的目标函数形式。
- 学习方法：Block结构支持并行化，支持out-of-core计算

## 16 XGBoost如何评价特征的重要性

- weight 该特征在所有树中被用作分割样本的特征的总次数。
- gain :该特征在其出现过的所有树中产生的平均增益
- cover：该特征在其出现过的所有树中的平均覆盖范围。

注意：覆盖范围是指一个特征用作分割点后，其影响的样本数量，即有多少样本经过该特征分割到两个子结点。

## 17 XGBoost参数调优的一般步骤

初始话一些基本变量，例如:

- max_depth = 5
- min_child_weight = 1
- gamma = 0
- subsample,colsample_bytree=0.8
- scale_pos_weight = 1

**(1) 确定learning rate 和estimate的数量**

**(2)max_depth和min_child_weight**

我们调整这两个参数是因为，这两个参数对输出结果的影响很大。我们首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。

max_depth，每棵子树的最大深度，check from range(3,10,2)。

min_child_weight，子节点的权重阈值，check from range(1,6,2)。

如果一个结点分裂后，它的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。

**(3) gamma** 最小划分损失，min_split_loss (0.1,0.5) 指对于一个叶子节点，当对它采取划分后损失函数的降低值的阈值

**(4) subsample ，cosample_bytree**  (0.6,0.9)

- subsample 是对训练数据的采样比例
- cosubsample_bytree 是对特征的采样比例

**(5)正则化参数**

alpha 是L1正则化系数，try 1e-5, 1e-2, 0.1, 1, 100

lambda 是L2正则化系数

**(6)降低学习率** 同时要增加树的数量，通常最后学习率为0.01-0.1

## 18 XGBoost模型如果过拟合了怎么解决

- 直接控制模型的复杂度 max_depth,min_child_weight,gamma 等参数
- 增加随机性，从而使得模型在训练时对于噪声不敏感 subsample 和cosample_bytree
- 直接减小学习率，但同时增加迭代次数 estimator

## 19 为什么XGBoost相比于某些模型对缺失值不敏感

树模型对缺失值的敏感度低，大部分时候可以在数据缺失时使用，原因是，一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点(特征值)，完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本特征值缺失，对寻找最佳分割点的影响不是很大。