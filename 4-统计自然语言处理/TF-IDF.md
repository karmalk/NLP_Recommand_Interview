# TF-IDF

---

[TOC]

## 什么是TF-IDF？

TF-IDF是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。

## TF-IDF

**一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章.**

- **TF:** Term Frequency, 表示词频。 一个给定的词在该文章中出现的次数。
  $$
  TF = \frac{\text{某个词在文章中的出现次数}}{\text{文章的总词数}}  \\
  $$

- **IDF:** Inverse Document Frequency, 表示逆文档频率。如果包含词条 t 的文档越少, IDF越大，则说明词条具有很好的类别区分能力。

$$
IDF = log(\frac{语料库的文档总数}{包含该词的文档数+1})  \\
$$

- **TF-IDF：**某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语
  $$
  \text{TF-IDF} = TF \times IDF
  $$
  

## 举例说明

假设现在有一篇文章， 文章中包含 10000 个词组， 其中，"贵州" 出现100次，"的" 出现500次，那么我们可以计算得到这几个词的 TF(词频) 值：
$$
TF(贵州) = 100 / 10000 = 0.01 \\
TF(的) = 500 / 10000 = 0.05
$$
现在语料库中有 1000 篇文章， 其中，包含 "贵州" 的有 99 篇， 包含 "的" 的有 899 篇， 则它们的 IDF 值计算为：
$$
IDF(贵州) = log(1000 / (99+1)) = 1.000 \\
IDF(的) = log(1000 / (899+1)) = 0.046
$$

## 优缺点

- 优点简单快速，而且容易理解。
- 缺点是有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。

