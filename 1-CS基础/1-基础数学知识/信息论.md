# 信息论

tags: 基础数学知识

---

[TOC]

## 基本知识

- 基本思想: 一件不太可能的事情发生, 要比一件非常可能的事情发生提供更多的信息

- 性质:

  > - 非常可能发生的事情信息量较少,并且极端情况下,一定能够发生的事件应该没有信息量
  > - 比较不可能发生的事件具有更大的信息量
  > - 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。

## 1. 自信息 ，信息熵，互信息

### 自信息 - self-information

如果说概率P是对确定性的度量，信息是对不确定性的度量，这两者是相对的， **事件发生的概率越大，那么事件的信息量就越小， 事件的概率与事件的信息量之间成反比。**

举例来说：如果**事件A发生的概率比事件B发生的概率要大**，那么我们就说**事件B的信息量要比事件A的信息量要大**。

信息量能够量化以上性质,定义一个事件x的自信息为：
$$
I(x) = -log(p(x))
$$
当该对数的底数为自然对数 e 时，单位为奈特（nats）；当以 2 为底数时，单位为比特（bit）或香农（shannons）.

### 信息熵 -- information-entropy

信息熵是对**平均不确定性**的度量，本质上是**所有事件的信息量的期望**， 对整个概率分布中的不确定性总量进行量化：

$$
H(X) = E_{X}[I(x)]=-\sum_{x \in X} p(x)log(p(x))； \quad X 表示所有事件\\
$$
信息论中，记 `0log0 = 0`

- 当且仅当某个 $P(X_i)=1$，其余的都等于0时， H(X)= 0。
- 当且仅当某个$P(X_i)=1/n，i=1， 2，……， n$时，$H(X)$ 有极大值 log n。

熵可以表示样本集合的不确定性，**熵越大，样本的不确定性就越大**。

### 互信息

$$
I(X,Y) = \sum_{y \in Y} \sum_{x \in X} p(x,y) log( \frac{p(x,y)}{p(x)p(y)})
$$

互信息 $I(X,Y)$ 取值为非负。当X、Y相互独立时，$I(X,Y)$ 最小为0。

## 2. 相对熵（KL散度） 与 交叉熵

### 1. 相对熵 -- KL 散度 ： Kullback-Leibler divergence

如果对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度来衡量**这两个分布的差异**。

- 定义： P 对 Q 的KL散度为：

$$
D_P(Q) =\sum_{x \in X}P(x)log(\frac{P(x)}{Q(x)})
$$

- 含义：在离散型变量的情况下， KL 散度衡量的是：**当我们使用一种被设计成能够使得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号的消息时，所需要的额外信息量。**

- 性质：

  > - **非负: **KL 散度为 0 当且仅当P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎处处”相同的.
  > - **不对称**：$D_p(q) != D_q(p)$

### 2. 交叉熵 - cross entropy

- 设 $p(x), q(x)$ 为 $X$ 中取值的两个概率分布，则 $p$ 对 $q$ 的交叉熵为：

$$
D(p || q) = -\sum_{x \in X}p(x)log\, \frac{p(x)}{q(x)}
$$

在一定程度上，相对熵可以度量两个随机变量的“距离”。

### 3. 交叉熵与KL散度的关系

- **针对 Q 最小化交叉熵等价于最小化 P 对 Q 的 KL 散度**，因为 Q 并不参与被省略的那一项。
  $$
  H_P(Q) = H(P) + D_P(Q)最大似然估计中，最小化 KL 散度其实就是在最小化分布之间的交叉熵。
  $$

- 最大似然估计中，最小化 KL 散度其实就是在最小化分布之间的交叉熵。

## 3. 联合熵与条件熵

- 联合熵 $H(X, Y)$：两个随机变量X，Y的联合分布。

- 条件熵 $H(Y|X) $：在随机变量X发生的前提下，随机变量Y发生所新带来的熵定义为Y的条件熵，用来衡量在已知随机变量X的条件下随机变量Y的不确定性。

$$
H(Y|X) = H(X,Y) - H(X)
$$

联合熵与条件熵的推导过程如下：
$$
\begin{align}
H(X, Y) - H(X) &= -\sum_{x,y} p(x,y) log \, p(x,y) + \sum_x p(x) log \, p(x) \\
&= -\sum_{x,y} p(x,y) log \, p(x,y) +  \sum_x (\sum_y p(x,y)) \, log \, p(x) \qquad \text{边缘分布 p(x) 等于联合分布 p(x,y) 的和} \\
&= -\sum_{x,y} p(x,y) log \, p(x,y) +  \sum_{x,y} p(x,y) \, log \, p(x) \\ 
&= -\sum_{x,y} p(x,y) log \frac{p(x,y)}{p(x)} \\
&= -\sum_{x,y} p(x,y) log p(y|x)
\end{align}
$$

## 4. 互信息

- $I(X, Y)$ ：两个随机变量X，Y的互信息 为**X，Y的联合分布**和**各自独立分布乘积**的**相对熵**。

$$
I(X, Y) = \sum_{x,y} p(x,y) log \frac{p(x,y)}{p(x)p(y)} \\
I(X, Y) = D(P(X,Y) || P(X)P(Y))
$$

推导如下：

<https://www.nowcoder.com/ta/review-ml/review?page=59>