## 1. 矩阵相乘

$$
c_{ij} = a_{ik} * b_{kj}
$$





## 1. 范数

### 1. 向量的范数

任意一组向量设为$\vec{x}=(x_1,x_2,...,x_N)$ 如下：

- 向量的1范数： 向量的各个元素的绝对值之和

$$
\Vert\vec{x}\Vert_1=\sum_{i=1}^N\vert{x_i}\vert
$$

- 向量的2范数： 向量的每个元素的平方和再开平方根

$$
\Vert\vec{x}\Vert_2=\sqrt{\sum_{i=1}^N{\vert{x_i}\vert}^2}
$$

- 向量的负无穷范数： 向量所有元素的绝对值中最小的
$$
\Vert\vec{x}\Vert_{-\infty}=\min{|{x_i}|}
$$

- 向量的正无穷范数： 向量所有元素的绝对值中最大的

$$
\Vert\vec{x}\Vert_{+\infty}=\max{|{x_i}|}
$$

- 向量的p范数： 向量元素绝对值的p次方和，然后再开P次方根
$$
L_p=\Vert\vec{x}\Vert_p=\sqrt[p]{\sum_{i=1}^{N}|{x_i}|^p}
$$

### 2. 矩阵的范数

对于矩阵 $A_{m \times n}$， 举例而说： 

```
A = [
-1, 2, 3;
4, -6, 6;
]
```

- 矩阵的1范数（列范数）：矩阵的每一列上的元素绝对值先求和，再从中取个最大值
$$
\Vert A\Vert_1=\max_{1\le j\le n}\sum_{i=1}^m|{a_{ij}}| \\
\text{举例}: \Vert A\Vert_1 = max([5,8,9]) = 9
$$

- 矩阵的2范数： 矩阵 $A^TA$ 的最大特征值开平方根
$$
\Vert A\Vert_2=\sqrt{\lambda_{max}(A^T A)}
$$



## 4. 特征值分解，特征向量

- 特征值分解可以得到特征值与特征向量
- 特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么

矩阵A 的特征值与其特征向量$\vec{v}$, 特征值 $\lambda$ 满足：
$$
A\nu = \lambda \nu
$$

特征值分解是将一个矩阵分解为如下形式：

$$
A=Q\sum Q^{-1} \\
Q: 矩阵A的特征向量组成的矩阵 \\
$$

$\sum$: 一个对角矩阵，每一个对角元素是一个特征值里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。 特征值分解表示矩阵$A$的信息可以由其特征值和特征向量表示。


## 6. 条件概率

$$
P(A|B) = \frac{P(A\cap B)}{P(B)} \\
P(A_1A_2...A_n)=P(A_n|A_1...A_{n-1})...P(A_2|A_1)P(A_1)
=P(A_1)\prod_{i=2}^{n}P(A_i|A_1...A_{i-1})
$$

## 7. 联合概率与边缘概率

- 联合概率：联合概率指类似于$P(X=a,Y=b)$这样，包含多个条件，且所有条件同时成立的概率。
- 边缘概率：边缘概率指类似于$P(X=a)$，$P(Y=b)$这样，仅与单个随机变量有关的概率。

- 区别：联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率。而边缘概率是某个事件发生的概率，而与其它事件无关。

- 联系： 联合分布可求边缘分布，但若只知道边缘分布，无法求得联合分布。  


## 8. 独立性与条件独立性

- 独立性：$P(XY)=P(X)P(Y)$
- 条件独立： 再条件Z发生时， X， Y 条件独立：
$$
X\bot Y|Z \iff P(X,Y|Z) = P(X|Z)P(Y|Z)
$$


## 8. 期望，方差，协方差， 相关系数

### 1. 期望

$$
E(ax+by+c) = aE(x)+bE(y)+c \\
离散函数期望：E(f(x))=\sum_{k=1}^{n}{f(x_k)P(x_k)} \\
连续函数期望：E(f(x))=\int_{-\infty}^{+\infty}{f(x)p(x)dx}
$$

**如果$X$和$Y$相互独立，则$E(xy)=E(x)E(y)$。  **

### 2. 方差

$$
Var(x) = E((x-\overline{x})^2) \\
Var(x) = E(x^2) -E(x)^2
$$


**如果$X$和$Y$相互独立,$Var(ax+by)=a^2Var(x)+b^2Var(y)$**

### 3. 协方差

**协方差是衡量两个遍历的总体误差。**

$$
Cov(x,y)=E((x-E(x))(y-E(y))) \\
Cov(ax,by) = abCov(x,y) \\
Cov(a+bx, c+dy) = bdCov(x, y)
$$

**独立变量的协方差为0。**

### 4. 相关系数

$$
Corr(x,y) = \frac{Cov(x,y)}{\sqrt{Var(x)Var(y)}}
$$

**值越接近1，说明两个变量正相关性（线性）越强。越接近-1，说明负相关性越强，当为0时，表示两个变量没有相关性**

---

## QA

### 1. 矩阵正定性

- 问题：矩阵正定性的判断？ 

  > - 矩阵中的特征值均不小于 0 ， 则为半正定。
  > - 矩阵中的特征值都大于 0， 则为正定。

- Hessian矩阵正定性在梯度下降中的应用

  > - 在判断优化算法的可行性时Hessian矩阵的正定性起到了很大的作用,若Hessian正定,则函数的二阶偏导恒大于0,函数的变化率处于递增状态,在牛顿法等梯度下降的方法中,Hessian矩阵的正定性可以很容易的判断函数是否可收敛到局部或全局最优解。

