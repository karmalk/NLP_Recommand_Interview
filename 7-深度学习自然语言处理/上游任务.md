# 各大上游任务

[TOC]

# 文本分类模型

tags: 博客文章

------



## 1. TextCNN[1]

![](http://ww1.sinaimg.cn/large/006gOeiSly1g2iksw5rx2j30o70b0js1.jpg)

 $x_i \in R^k$ 表示一个第$i$ 个词其 $k$ 维的词向量表示， 对于一个长度为 $n$ 的句子，有：$X = \{x_1, \cdots, x_n\} \in R^{n \times k}$， 我们通过对向量矩阵 $X$ 进行卷积操作来提取特征， 其中， $x_{i:i+j}$ 表示第 $i$ 个词到第 $i+j$ 个词，共 $j+1$ 个词。

对于一个窗口大小为 $h$ 的卷积核， 其 shape 为 $w \in R^{h \times k}$ ， 其提取特征的过程为：
$$
c_i = f(w \cdot x_{i:i+h-1} + b); \quad b \in R， c_i \in R
$$
1个卷积核对 $X$ 一次卷积的过程需要对 $\{x_{1:h}, x_{2:h+1}, \cdots, x_{n-h+1:n}  \}$ 分别进行卷积操作， 我们得到最终的特征表示： 
$$
c = [c_1, c_2, \cdots, c_{n-h+1}]  ; \quad c \in R^{n-h+1}
$$
然后，文章对特征向量 $c$ 采用最大池化操作来提取最重要特征：
$$
\hat{c} = Maxpool(c); \quad \hat{c} \in R
$$
上述的过程描述的是一个卷积核对$X$ 提取特征的过程，而实际中，我们往往要采用多种窗口大小的卷积核，且每种窗口的卷积核有很多个，这里假设卷积核的窗口大小为 3， 4， 5， 卷积核的shape分别为 $3 \times k , 4 \times k, 5 \times k $， 其对应的卷积核数量为 $m_1, m_2, m_3$ 。

对于窗口大小为 3 的卷积核， 我们在一次卷积过后获得一个$C = (n-h+1) \times 1 \times m_1$的矩阵， 然后对该矩阵进行最大池化得到一个 $ 1 \times 1 \times m_1$  的向量， 该向量就是窗口为3 的卷积核所提取的全部特征。

同样的道理，窗口为 4 的卷积核所提取的特征为一个 $1 \times 1 \times m_2$ 的向量， 窗口为 5 的卷积核所提取的特征为一个 $1 \times 1 \times m_3$ 的向量。

最后我们将这三个向量拼接起来形成一个 $z \in R^{1 \times (m_1 + m_2 + m_3) }$ 的向量， 然后将该向量送入输出层：
$$
y = w \cdot (z \circ r) + b; \quad r \in R^{m_1+m_2+m_3} \quad \text{r为 dropout}
$$

## 2. 对TextCNN 的分析 [3]

文章 [3] 对CNN 用于文本分类时的超参进行分析，这些超参包括： 词向量的选择，Filter 的大小， 卷积核的数量， 激活函数的选择， Pooling 策略， 正则化方法。

**Word Embedding**  

文章比较了三种情况： Word2vec， Glove， Word2vec + Glove， 而实际上，三者的性能相差无几， 具体的依旧要看任务数据集，并没有定论，因此在实际的开发中，分别采用不同的预训练词向量来帮助我们更好的选择。

**Filter Size**

不同的数据集有其适合的 Filter Size， 文章建议区域大小为 **1-10** 内进行线性搜索， 但如果数据集中的句子长度较大(100+)， 那么可以考虑设置较大的 Filter Size。

不同size的 Filter 进行结合会对结果产生影响，当把与**最优 Filter size 相近的Filter 结合时**会提升效果，但如果与较远的Filter 结合会损害性能。因此，文章建议最初采用一个 Filter ， 调节 size 来找到最优的 Filter size， 然后探索最优Filter size的周围的各种 size 的组合。

**卷积核数量**

对于不同的数据集而言，卷积核的设置也有所不同，最好不要超过600，超过600可能会导致过拟合， 推荐范围为100-600。同时，卷积核数量增多，训练时间会变长，因此需要对训练效率做一个权衡。

**激活函数**

尽量多尝试激活函数， 实验表明，Relu， tanh 表现较佳。

**Pooling 策略**

实验分析得出， 1-max pooling 始终优于其他池化策略，这可能是因为在分类任务中，上下文的位置并不重要，且句子中的 n-granms 信息可能要比整个句子更具预测性。

**正则化方法**

实验表明，在输出层加上L2正则化并没有改善性能，dropout是有用的，虽然作用不明显，这可能是因为参数量很少，难以过拟合的原因所致。文章建议不要轻易的去掉正则化项，可以将 dropout 设置为一个较小值 (0-0.5)，推荐0.5 ， 对于L2， 使用一个相对较大的约束。 当我们增加卷积核数量时，可能会导致过拟合，此时就要考虑添加适当的正则项了。

## 3. TextRNN

![](http://ww1.sinaimg.cn/large/006gOeiSly1g2k36w52s9j30cz0ap0t9.jpg)

以双向LSTM 或GRU来获取句子的信息表征， 以最后一时刻的 h 作为句子特征输入到 softmax 中进行预测， 很简单的模型，就不详细介绍了。

## 4. TextRCNN [4]

说实话，这篇论文写的真乱，一个很简单的思想，看起来比 Transformer 还复杂，真的是有点醉， 不推荐看原论文，写的真的很冗余。 

文章的思想很简单：

- 首先，对于单词 $w_i$ ， 获得其词向量表示 $e(w_i)$
- 然后， 采用双向 GRU 来获取每个词的上下文向量表示 $c_l(w_i), c_r(w_i)$ 
- 为了更好的表示词的信息，文章将原始词向量 $e(w_i)$， 上下文表示$c_l(w_i), c_r(w_i)$  结合起来，形成词的新的向量表示，这里作者采用一个全连接网络来聚合这些信息：

$$
x_i = [c_l(w_i); e(w_i); c_r(w_i)] \\
y^{(2)} = tanh(W^{(2)} x_i + b^{(2)})
$$

- 采用最大池化来获取句子的最终表示：
  $$
  y^{(3)} = max_{i=1}^n y_i^{(2)}
  $$

- 最后，采用一个softmax 来做分类：

$$
y^{(4)} = W^{(4)} y^{(3)} + b^{(4)} \\
p_i = \frac{exp(y_i^{(4)})}{\sum_{k=1}^n exp (y_k^{(4)})}
$$

## 5. HAN [5]

![](http://ww1.sinaimg.cn/large/006gOeiSly1g2ipy7d8q6j30bo0dyq3f.jpg)

**问题定义**

HAN 主要针对 document-level 的分类， 假定document 中有L个句子：$\{s_1, ... s_L\}$， 对于句子 $s_i$， 其包含有 $T_i$ 个单词：$\{ w_{i1}, \cdots, w_{it}, \cdots w_{iT}\}$  。

**Word Encoder**

对于一个句子$s_i$ ，文章采用词向量矩阵将其做 Embedding， 然后采用双向 GRU 来获得该句子的上下文表示， 以第 $i$ 个句子中的第 $t$ 个单词为例：
$$
x_{it} = W_e w_{it}, t \in [1,T] \\
\overrightarrow{h}_{it} = \overrightarrow{GRU}_{(x_{it})},  t \in [1,T] \\
\overleftarrow{h}_{it} = \overleftarrow{GRU}_{(x_{it})},  t \in [T,1] \\
h_{it} = [\overrightarrow{h}_{it}, \overleftarrow{h}_{it}]
$$
**Word Attention**

考虑到在每个句子中，各个词对句子信息的贡献不同，因此此处引入一个注意力机制来提取语义信息，更好的获得句子的表示。
$$
u_{it} = tanh(W_w h_{it} + b_w) \\
\alpha_{it} = \frac{exp(u_{it}^Tu_w)}{\sum_t exp(u_{it}^Tu_w)}; \quad  u_w \text{是随机初始化的，并参与训练} \\
s_i = \sum_t \alpha_{it}h_{it}
$$
**Sentence Encoder**

一个 document 中有L个句子，我们需要对这L个句子的信息进行整合，但很明显，句子之间的信息是由关联的，因此文章采用双向GRU对句子信息进行综合来获得每个句子新的表示：
$$
\overrightarrow{h}_{i} = \overrightarrow{GRU}_{(s_i)}, i \in [1, L] \\
\overleftarrow{h}_{i} = \overleftarrow{GRU}_{(s_i)}, i \in [L, 1] \\
h_i = [\overrightarrow{h}_i, \overleftarrow{h}_i]
$$
**Sentence Attention**

考虑到在一个document中，各个句子的重要程度并不同，因此采用一个Attention 来对句子信息进行整合最终形成 document 的最终信息：
$$
u_i = tanh(W_sh_i + b_s) \\
\alpha_i = \frac{exp(u_i^T u_s)}{\sum_i exp(u_i^T u_s)}; \quad  u_s \text{是随机初始化的，并参与训练} \\
v = \sum_i \alpha_i h_i
$$
**Document Classification**
$$
p = softmax(W_c v + b_c) \\
L = -\sum_d log p_{dj}
$$

## DPCNN



 



## 最后

虽然文本分类是最简单的任务，但其在企业中应用最为广泛，十分适合初学者入门学习。

## Reference

[1] TextCNN： Convolutional Neural Networks for Sentence Classification

[3] A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification

[4] Recurrent Convolutional Neural Network for Text Classification

[5] Hierarchical Attention Networks for Document Classification

[n] Large Scale Multi-label Text Classification With Deep Learning



# 上游任务 - 文本匹配

tags: NLP





500 questions ： 18.5.6 如何做语义匹配？

------

[TOC]

## 前言

文本匹配算是一个基础性的任务，其可以应用到其他上游任务中如：信息检索，问答系统，对话等，这些上游任务本质上还是文本匹配的机制，只不过关注的核心在于，不同的任务需要不同的匹配机制。

## 文本匹配简介

### 1. 文本匹配的挑战

- 词语匹配的多元性： 不同词可以表示同一个语义； 同一个词在不同语境下会有不同的语义。
- 短语匹配的结构性：中文中这种词非常常见， 如：“机器学习” 与 “学习机器” 是两个不同的概念
- 文本匹配的层次性：文本是以层次化的方式组织起来的，词语组成短语，短语组成句子，句子形成段落，段落形成篇章，在设计模型时，如何考虑不同层次的匹配信息是十分重要的。

### 2. 深度学习文本匹配模型

深度学习在文本匹配模型中的应用大致可分为三类：

- 基于单语义文本表达： 将单个文本表达成一个稠密向量，然后计算两个向量之间的相似度来作为文本的匹配度[1][2]
- 基于多语义文本表达：





## 1. 单粒度语义文本表达

十分适合信息检索这种对存储和速度要求都较高的任务

- 优点：

  > - 将文本映射为一个简洁的表达， 便于储存
  > - 匹配的计算速度快



DSSM

CDSSM -- 卷积

ARC-I -- 卷积

CNTN -- 卷积

LSTM-RNN

## 2. 多粒度语义文本表达

MultiGranCNN



<https://github.com/NTMC-Community/awesome-neural-models-for-semantic-match>

<https://github.com/sebastianruder/NLP-progress/blob/master/english/semantic_textual_similarity.md>



## MANM



## Reference

[1]  Siamese Recurrent Architectures for Learning Sentence Similarity

[2]  Learning Text Similarity with Siamese Recurrent Networks

[2]  深度文本匹配综述

[3] 



# 上游任务 - 知识图谱



## 简介

知识图谱可以形式化表示为： O = {C, H, P, A, I}， 其中有：

- C：概念集合，如事物性概念和事件类概念
- H： 概念的上下位关系集合
- P：属性集合，用于描述概念所具有的特征
- A： 规则集合，描述领域规则
- I：实例集合，描述实例-属性-值

## 发展历程

![1](D:/算法岗面试学习之路/NLPer-Interview-master/img/知识图谱/1.jpg)



## 知识图谱技术

![2](D:/算法岗面试学习之路/NLPer-Interview-master/img/知识图谱/2.png)

知识图谱技术包含三部分，分别是知识图谱构建技术，知识图谱查询和推理技术以及知识图谱应用。

- 知识图谱构建技术：知识表示，实体识别，实体链接， 实体关系学习，事件知识学习
- 知识图谱查询和推理技术：知识存储和查询，知识推理
- 知识图谱应用：通用和领域知识图谱，语义集成，语义搜索，基于知识的问答

### 1. 知识图谱构建技术

#### 1. 知识表示

知识的表示形式，是字符串还是低维向量，目前来看，发展最好的是采用深度学习技术来做实体的表示学习以及关系的表示学习，类似Word2Vec。最终将实体和关系表示为稠密的低维向量。

#### 2. 实体识别

实体识别的目的是识别文本中指定类别的实体，从而为知识图谱构建合适的实体集。

#### 3. 实体链接

实体链接是文本中识别出的实体与知识库中对应的实体进行链接，从而实现构建知识图谱。

#### 4. 实体关系学习

实体关系学习就是自动从文本中检测和识别出实体之间具有的某种语义关系，也称为关系抽取。

实体关系抽取分为两类：

- 预定义关系抽取：系统所抽取的关系是预先定义好的，比如知识图谱中定义好的关系类别，如上下位关系、国家—首都关系等。
- 开放式关系抽取：开放式关系抽取不预先定义抽取的关系类别，由系统自动从文本中发现并抽取关系。

#### 5. 事件知识学习

事件是促使事物状态和关系改变的条件，是动态的、结构化的知识。目前已存在的知识资源（如谷歌知识图谱）所描述多是实体以及实体之间的关系，缺乏对事件知识的描述。针对不同领域的不同应用，事件有不同的描述范畴。一种将事件定义为发生在某个特定的时间点或时间段、某个特定的地域范围内，由一个或者多个角色参与的一个或者多个动作组成的事情或者状态的改变。一种将事件认为是细化了的主题，是由某些原因、条件引起，发生在特定时间、地点，涉及某些对象，并可能伴随某些必然结果的事情。事件知识学习，即将非结构化文本文本中自然语言所表达的事件以结构化的形式呈现，对于知识表示、理解、计算和应用意义重大。

### 2. 知识图谱查询和推理计算

#### 1. 知识存储与查询

知识图谱以图的方式来展现实体、事件及其之间的关系。知识图谱存储和查询研究如何设计有效的存储模式支持对大规模图数据的有效管理，实现对知识图谱中知识高效查询。

#### 2. 知识推理

知识推理从给定的知识图谱推导出新的实体跟实体之间的关系。知识推理可分为两部分：

- 基于符号的推理
- 基于统计的推理：通过统计规律从知识图谱中学习到新的实体间关系。

### 3. 知识图谱应用

#### 1.  通用和领域知识图谱

- 通用知识图谱：面向通用领域，可以看做一个面向通用领域的结构化的百科知识库，其中包含大量的现实世界中的常识性知识，覆盖面广。
- 领域知识图谱：面向某一特定领域，可以看成是一个基于语义技术的行业知识库，因其基于行业数据构建，有着严格而丰富的数据模式，所以对该领域知识的深度、知识准确性有着更高的要求。

#### 2. 语义集成

语义集成目的是将不同知识图谱融合为一个统一、一致、简洁的形式，为使用不同知识图谱的应用程序间的交互提供语义互操作性。

#### 3. 语义搜索

语义搜索：基于实体和关系的检索。语义搜索利用知识图谱可以准确地捕捉用户搜索意图，借助于知识图谱，直接给出满足用户搜索意图的答案，而不是包含关键词的相关网页的链接。

#### 4. 基于知识的问答

问答系统返回的不在是基于关键字匹配的相关文档排序，而是精准的自然语言形式的答案。

以直接而准确的方式回答用户自然语言提问的自动问答系统将构成下一代搜索引擎的基本形态。



# 上游任务 - 阅读理解

------



## 1. Attentive Reader [1]

![](http://ww1.sinaimg.cn/large/006gOeiSly1g164kt342sj30bu07qaa9.jpg)

Attentive Reader 就有点常规模型的样子了。 

- 首先，采用**双向LSTM**分别对 passage 与 query 进行 Embedding 来获得上下文表示；

  其中，对于 passage  而言，其获得的是一个矩阵 y，矩阵的每一列是 passage 中词的上下文表示； 

  对于 query， 其将整个信息压缩为一个向量 u。 

- 其次是注意力机制的使用，这里的 Q 为 passage的表示 y， Key 为 query的表示 u， 这里的注意力机制计算公式为：
  $$
  相似度/相关度计算公式： m(t) = tanh(W_{ym} y(t) + W_{um} u)    \\
  注意力权重值计算： s(t) = \frac{e^{W^T_{ms} m(t)}}{\sum_{t=1}^{|p|} e^{W^T_{ms} m(t)}} \\
  最终表示： r = ys
  $$

- 最后是以 r， u 作为接下来模型 output layer 的输入来进行预测。

但可能会存在一个问题，如果 query 的长度也很长，那么压缩成一个向量，其信息的损失不得不考虑进去。

## 2. Standford Reader[2]

![](http://ww1.sinaimg.cn/large/006gOeiSly1fxdmog4989j30kf0cfmyd.jpg)

该模型是 **Attentive Reader** 延伸， 但在 Attention 的部分又有所不同。

- 首先，模型通过 **双向LSTM** 分别对 Passage 与 Query 进行Embedding， 对于 Passage， 我们获得一个词的上下文信息矩阵：$\tilde{p} \in R^h $； 而对于 Query， 我们获得一个句子级别的向量表示： $q \in R^h$。  

- 接下来，我们需要计算 Passage 中每个词与 Query的相关度， 然后获得最终输出到output layer 的输出表示：
  $$
  \alpha_i = softmax_i \, q^T W_s \tilde{p}_i ; \quad W_s \in R^{h \times h } \\
  o = \sum_i \alpha_i \tilde{p}_i
  $$

- 最后，我们将 $o$  输入到 output layer， 然后进行预测：
  $$
  a = argmax \, W^T_a o
  $$



很明显，该模型更加干净，简洁，且获得的效果是要比 **Attentive Reader** 好 8-10% 个点的。 我们来简单介绍一下二者的不同：

- 首先，在score部分计算相关度时， Attentive Reader 采用的是 tanh， 而该模型中采用的是MLP， 这样不仅仅效果更好，也更高效。 
- 其次， 在 output layer 部分，Attentive Reader 在最终预测之前，采用 tanh 将 $r$ 与 $u$ 结合起来； 而Standford Reader 仅仅采用 $o$ 作为最终预测的输入， 这种用法更加高效。
- 考虑到预测的结果是一个会出现在 Passage 中的实体，因此 Standford Reader 仅在出现的实体范围内进行预测。而 Attentive Reader 会在Passage， Question 中出现所有单词中进行预测。

## 3. Attention Sum Reader [3]

![](http://ww1.sinaimg.cn/large/006gOeiSly1g1710nay1rj30us0ix76a.jpg)

其实，该模型与上面的Standford Reader 很像，但加了一个小 trick。

- 首先，采用双向 GRU 分别对 Document 与 Question 进行 Embedding； 对于Document， 我们获得了一个上下文Embedding 矩阵 $f$ ；对于 Question， 我们获得了一个 sentence-level 句子向量$g$。

- 然后，计算 Document 中每个词与 Question 的相关度，这里采用点积的方式来做：
  $$
  s_i = softmax(f_i \cdot g)
  $$

- 最后，考虑到 Document 中同一个词可能会出现多次，因此这里将相同词的注意力权重相加得到该词最终的注意力权重。
  $$
  Attention \, weight(word) = \sum_{i \in I(w,d)} s_i \\
  I(w,d) \, 是 \, word \, 出现在 \, document \, 中位置的集合
  $$

- 最后，我们从实体选项中选择 Attention weight 最高的作为答案。

 但是，分析一下那个Attention Sum 操作， 其源于作者观察到答案更多偏爱出现次数较多的词，这说明，该 trick 是任务偏好的，并没有很广泛的应用价值， trick 有取巧的嫌疑。

## 4. Attention-over-Attention [4]

![](http://ww1.sinaimg.cn/large/006gOeiSly1g1729bn779j30t30kdabj.jpg)

Attention-over-Attention 这篇文章质量很高，在后续的很多模型中都有使用，创新度也很高，**值得精读。**

- 首先， 依旧是使用**双向RNN（LSTM or GRU)** 来获得 Document 与 Query 的上下文表示矩阵： $h_{doc} \in R^{|D| * 2d}; \quad h_{query} \in R^{|Q| * 2d}$。 

- 然后，我们计算 **Pair-wise Matching Score**，其实就是计算 Document 中**第 i 个词** 与 Query 中的**第 j 个词**的相似度或相关度：
  $$
  M(i, j) = h_{doc}(i)^T \cdot h_{query}(j) 
  $$

- 再然后，进行 **Individual Attentions**， 其实就是对**矩阵 M 的每一列**做 softmax， 其中，M的一列代表的是**对于 Query 中的某个词与所有 Document 中词的相关度**， 那么对每一列进行 softmax 的意思就是对于给定的一个 Query 词， 对 Document 中的每个词进行 Attention，这里称之为 **query-to-document  attention**， 公式如下：
  $$
  \alpha(t) = softmax(M(1,t), \cdots, M(|D|, t)); \quad a(t) \in R^{|D|}  \\
  \alpha = [\alpha(1), \alpha(2), \cdots, \alpha(|Q|) ]；\quad \alpha \in R^{|Q|\times |D|}
  $$

- 然后，进行 **Attention-over-Attention**， 其实就是对**矩阵M的每一行做 softmax**， 而 **M 的一行表示的是对于 Document 中的某个词与所有 Query 中词的相关度**，那么对每一行进行softmax 的意思就是对于给定的一个Document 词，对Query 中的每个词进行Attention， 这里称为 **document-to-query attention**， 公式如下：
  $$
  \beta(t) = softmax(M(t,1), ... M(t, |Q|)); \quad \beta(t) \in R^{|Q|}
  $$
  然后， 对 $\beta(t)$ 求和平均来得到 query-level attention $\beta$， 从直观上而言，这里是获得对于整个document，每个query的Attention value:
  $$
  \beta = \frac{1}{n} \sum_{t=1}^{|D|} \beta(t); \quad \beta \in R^{|Q|}
  $$
  最后，我们将 $\alpha$ 与 $\beta$ 做点乘得到 **attention document-level attention**:
  $$
  s = \alpha^T \cdot \beta \quad ; \quad s \in R^{|D|}
  $$

- 最终，Final Predictions 将相同词的score 合并，得到每个词的score， 其实就是Attention-Sum里面提出的创新部分：
  $$
  P(w | D, Q) = \sum_{i \in I(w,D)} s_i, \quad w \in V
  $$



本模型可以说是花式Attention的一个典型代表了，其不仅仅考虑了query到document的attention，而且考虑了document 到 query 的attention，于是称为 attention over attention。 虽然无法解释其内部的玄学，但的确该机制在很多后续的模型中都有应用，效果也不错。

## 5. Gated-Attention Reader [5]

![](http://ww1.sinaimg.cn/large/006gOeiSly1g17xe43o1oj30qh0d3jsi.jpg)

- 首先，采用双向RNN（GRU） 来获得 Document 与query的上下文表示矩阵，表示如下：
  $$
  Document : X^{(0)} = [x_1^{(0)}, x_2^{(0)}, \cdots ,  x_{|D|}^{(0)}] \\
  Query: Y = [y_1, y_2, \cdots , y_{|Q|}] \\
  Document: D^{(1)} =  \overleftrightarrow{GRU}_D^{(1)}(X^{(0)}) \\
  Query: Q^{(k)} =  \overleftrightarrow{GRU}_Q^{(k)}(Y)
  $$
  原论文中的图好像画错了，$D^{(k)}$ 前不应该是 $X^{(k-1)}$ 吗？不过倒是不影响理解。

- 然后，在接下来计算中，我们要不断的迭代 D 与 X：
  $$
  D^{(k)} =  \overleftrightarrow{GRU}_D^{(k)}(X^{(k-1)}) \\
  X^{(k)} = GA(D^{(k)}, Q^{(k)})
  $$
  其中， GA Attention 的计算公式如下：
  $$
  \alpha_i = softmax(Q^T d_i) \\
  \tilde{q_i} = Q \alpha_i \\
  x_i = d_i \odot \tilde{q_i} \quad or \quad  x_i = d_i + \tilde{q}_i \quad or \quad  x_i = d_i || \tilde{q}_i \\
  $$
  从直观上看，其实还是不断的融入 query 信息来获得在 document 中与 query 最相关的实体词 。与上述几个模型来比较，该模型是多层的，更能够把握这种相关语义。

  这个过程，我们迭代了K次，最终得到了 $D^{(k)}$ 。 

- 在Answer Prediction 阶段，先找到空白处位置的词的表示， 然后与 $D^{(k)}$ 做内积，再进行softmax：
  $$
  q_l^{(K)} = q_l^f || q_{T-L+1}^b \quad ?? \quad  这个不太懂\\
  s = softmax((q_l^{(K)})^T D^{(K)})
  $$

- 最后，再将相同词的概率合并：
  $$
  Pr(c | d,q ) = \sum_{i \in I(c,d)} s_i
  $$

## 6. BiDAF [6]

![](http://songyingxin-img.oss-cn-beijing.aliyuncs.com/19-1-4/32443699.jpg)

- 首先，采用char-level 与 word-level 信息，通过 Highway Networks 融合信息来分别获得 Context 与 Query 的矩阵表示：
  $$
  X \in R^{d \times T} ; \text{T 为 context 中的单词数}\\
  Q \in R^{d \times J} ; \text{J 为 Query 中的单词数}
  $$

- 然后，通过双向 LSTM 来获得每个词的上下文表示：
  $$
  H \in R^{2d × T} ; \text{Context 的上下文表示}\\
  U \in R^{2d × J} ；\text{Query 的上下文表示}
  $$

### 核心点： 双向注意力机制

双向注意力机制包括 Context-to-Query 与 Query-to-Context 两大部分：

- 首先，我们先计算 H 与 U 的相似矩阵：$S\in R^{T × J}$， 对于context 中的第 $t$ 个单词与 query 中的第 $j$ 个单词，有：
  $$
  S_{tj} = \alpha({H_{:t}, U_{:j}}) = \alpha(h,u) = W_{(S)}^T[h;u;h \circ u] \\
  H_{:t}：\text{context 的第 t 列} \\
  U_{:j}：\text{Query 的第 j 列}
  $$



- **Context-to-query Attention（C2Q）：**其含义是对于 Context 中的第 $t$ 个单词， 我们计算 Query 中的每个单词与该词的相关度，然后进行加权求和将 Query 的信息融合入 Context。 

  我们用 $a_t \in R^J$ 来表示对于单词 $t$ 的这种相关度：
  $$
  a_t = softmax(S_{t:}) \in R^J ; \quad S_{t:}: \text{S 的第t行}\\
  $$
  然后我们通过这些相关度信息来将计算 Context 中每个词的新表示：
  $$
  \hat{U}_{:t} = \sum_j a_{tj}U_{:j} \in R^{2d \times T} \\
  \hat{U}_{:t}: \hat{U} 的第 t 列 \\
  U_{:j}: U 的第j列
  $$

- **Query-to-context Attention（Q2C）**：其本质是计算对于 Query 中的词， Context 中的每个词与它的相关度， 然后通过加权求和将 Context 的信息融入到 Query 中。 而此段中的计算与上述有所不同：
  $$
  b = softmax(max_{col}(S)) \in R^T; \quad
  $$
  上式的含义是先取 S 中每一列的最大值形成一个新的向量 ， 然后对这个新的向量求相关度， 其实意思对于 Query 的整体信息， Context 中每个词对其的相似度分配， 然后我们计算对于Query来说，Context 中的word信息：
  $$
  \hat{h} = \sum_t b_t H_{:t} \in R^{2d}
  $$
  然后 $\hat{h}$ 重复 T 次形成 T 列， 形成 $\hat{H} \in R^{2d \times T}$， 其实就是类似 Q2C 矩阵。 这里有一点疑问的是，为何不像上文一样计算， 是考虑到计算复杂度吗？

- 此时，我们将这些信息综合， 其实就我看来就是将 Query 的信息融入到 Context 中，如下：
  $$
  G_{:t} = \beta(H_{:t}, \hat{U}_{:t}, \hat{H}_{:t}) \in R^{d_G}
  $$
  其中， $\beta$ 可以选择多种方式，如多层感知机或者仅仅简单的将各个向量连接。 

## 7. DCN [7]

![1](D:/算法岗面试学习之路/NLPer-Interview-master/img/DCN/1.PNG)

- 同样先通过 LSTM 来分别获得 Context 与 Query 的表示：
  $$
  D = LSTM(Context) \in R^{|D| \times dim}\\
  Q' = LSTM(Query);  \in R^{|Q| \times dim}\\ 
  Q = tanh(Q^{(Q)}Q' + b^{(Q)}) \in R^{|Q| \times dim} ; 可忽略
  $$

### 核心： CoAttention Encoder

$$
L = D^TQ \in R^{|D| \times |Q|}\\
A^Q = softmax(L) \in R^{|D| \times |Q|}; \quad C^Q = DA^Q \in R^{dim \times |Q|} \\
A^D = softmax(L^T) \in R^{|Q| \times |D|}; \quad C^D = [Q; C^Q]A^D \in R^{2dim \times |D|} \\
U = LSTM([D; C^D])
$$

## 8. QANet [8]

![2](D:/算法岗面试学习之路/NLPer-Interview-master/img/QANet/2.PNG)

- 首先，采用char-level 与 word-level 信息，通过 Highway Networks 融合信息来分别获得 Context 与 Query 的矩阵表示：
  $$
  X \in R^{d \times T} ; \text{T 为 context 中的单词数}\\
  Q \in R^{d \times J} ; \text{J 为 Query 中的单词数}
  $$

### 核心点： 多维信息融合

与 BIDAF 中不同， 其在信息的表示层面做了一个很精巧的融合，采用多个如下图的 Encoder Block 来获得文本的表示：

![1](D:/算法岗面试学习之路/NLPer-Interview-master/img/QANet/1.jpg)

整个 Encoder Block 的结构是： 卷积层 + Self-Attention 层 + Feed-forward 层。 作者认为，卷积能够捕获文本的局部结构，而 Self-Attention 能够学习到全局的相互特征， 且最大的优点在于二者皆可并行。

最终得到 Context 与 query 的矩阵表示：$C$ 与 $Q$。 

### Context-Query Attention Layer

$$
S = f(C,Q) \in R^{n \times m} \\
S_列 = softmax(S) \in R^{n \times m} \\
\text{context-to-query attention:} \qquad A = \overline{S} \cdot Q^T \in R^{n \times d} \\
S_行 = softmax(S) \in R^{n \times m} \\
\text{query-to-context attention:} \qquad B = S_列 \cdot S_行^T \cdot C^T \\

U = [C, A, C \odot A, C \odot B]
$$

## Reference

[1]  Teaching Machines to Read and Comprehend

[2]  A thorough examination of the cnn/dailymail reading comprehension task.

[3]  Text understanding with the attention sum reader network

[4]  Attention-over-Attention Neural Networks for Reading Comprehension

[5]  Gated-attention readers for text comprehension

[6] BI-DIRECTIONAL ATTENTION FLOWFOR MACHINE COMPREHENSION

[7] Dynamic coattention networks for question answering

[8] QANET: COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF-ATTENTION FOR READING COMPREHENSION

# 上游任务 - 关系抽取

---

....

# 上游任务 - 开放域对话

tags: NLP

------



https://zhuanlan.zhihu.com/p/75009749

# 上游任务 - 任务型对话

tags: NLP

------



## Reference

综述 -- <https://time.geekbang.org/special/AI-voice>



# 上游任务 - 实体识别

------

.....

# 上游任务 -- 信息检索



<https://www.msra.cn/zh-cn/news/features/ming-zhou-nlp-search-engine>



# 上游任务 - 自然语言推理



