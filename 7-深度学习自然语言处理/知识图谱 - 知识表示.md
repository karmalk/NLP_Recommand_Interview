# 知识图谱 - 知识表示

---

[TOC]

## 1. 两大表示形式

知识表示分为两大类： 基于符号逻辑的知识表示 与 基于向量的知识表示。近年来，知识图谱的知识表示方法已经发生了很大的变化，主要受到以下两个方面的影响：

- **数据规模大。**现代知识图谱往往规模十分大，因此采用以三元组为基础的较为简单实用的知识表示方法，并弱化了对强逻辑表示的要求；
- **深度学习技术。**由于知识图谱是很多搜索、问答和大数据分析系统的重要数据基础，基于向量的知识图谱表示使得这些数据更加易于与深度学习模型集成，使得基于向量空间的知识图谱表示得到越来越多的重视。

### 1. 基于符号逻辑的知识表示

- 优点：基于显性知识表示，因而表示能力强，能处理较为复杂的知识结构，具有可解释性，并支持复杂的推理。

- 缺点：**计算效率问题**与**数据稀疏问题**。

### 2. 基于向量的知识表示学习

- 优点：易于捕获隐性知识， 并易于与深度学习模型集成；显著提升计算效率；有效缓解数据稀疏；实现异质信息融合。
- 缺点：对复杂知识结构的支持不够，可解释性差， 不能支持复杂推理。

## 2. 知识表示的挑战

### 1. 复杂关系建模

知识图谱中关系可划分为： 

- 1-1： 1个头实体对应1个尾实体
- 1-N：1个头实体对应多个尾实体
- N-1： 1个尾实体对应多个头实体
- N-N：N 个尾实体对应多个头实体

其中， 1-N， N-1， N-N 称为复杂关系。

### 2. 多源信息融合

知识图谱中依旧有大量与知识有关的其他信息没有被有效利用：

- 知识图谱中的其他信息， 如实体和关系的描述信息，类别信息等
- 知识图谱外的海量信息，如互联网文本包含大量与知识库实体和关系有关的信息。

### 3. 关系路径建模

知识图谱中，多步的关系路径能够反映实体之间的语义信息。如何突破知识表示学习孤立学习每个三元组的局限性，充分考虑关系路径信息是知识表示学习的关键问题。

## 2. 知识表示学习

知识表示学习是面向知识库中的实体和关系进行表示学习。该技术可以在低维空间中高效计算实体和关系的语义联系，有效解决数据稀疏问题，使知识获取，融合和推理的性能得到显著提升。

在进行详细方法介绍前，我们将知识库表示为：
$$
G = (E,R,S)
$$

- $E = \{ e_1, ..., e_{|E|} \}$ ： 表示知识库的实体集合，其中包含 $|E|$ 种不同实体。
- $E = \{ r_1,...r_{|R|} \}$：表示知识库的关系集合，其中包含 $|R|$ 种不同关系。
- $S  \subseteq E \times R \times E$ ： 表示知识库中三元组集合
- $(h, r, t)$：单一的三元组的表示， $h$ 表示头实体， $t$ 表示尾实体， $r$ 表示 $h$ 与 $r$ 的关系。

从表示方法上看，几个代表模型可分为： 距离模型，单层神经网络模型，能量模型，双线性模型，张量神经网络模型，矩阵分解模型和翻译模型等。

### 1. 距离模型 - SE

结构表示(structured embedding，SE)中，每个实体用 $d$ 维向量表示，所有实体在同一个向量空间。并为每个关系 $r$  定义两个矩阵 $M_{r,l}, M_{r,2} \in R^{d \times d}$，用于三元组中头实体和尾实体的投影操作。

损失函数定义为：
$$
f_r(h,t) = |M_{r,l} l_h - M_{r,2}l_t|_{L_1}
$$
SE 通过两个矩阵将头实体与尾实体分别投影到关系 $r$ 的对应空间中，然后再该空间中计算两投影向的距离。 该距离表示 2 个实体在关系 $r$ 下的语义相似度，它们的距离越小，说明二者存在关系 $r$。

重要缺陷： 它对头，尾实体使用 2 个不同的矩阵进行投影，协同性较差，往往无法精确刻画两实体与关系之间的语义联系。

### 2. 单层神经网络模型 - SLM

单层神经网络模型尝试是采用单层神经网络的非线性操作来减轻 SE 无法协同精确刻画实体与关系的语义联系的问题。

损失函数定义为：
$$
f_r(h,t) = u_r^T g(M_{r,1}l_h + M_{r,2}l_t)
$$

- $M_{r,1}, M_{r,2} \in R^{d \times k}$： 投影矩阵
- $g()$： tanh 函数
- $u_r^T \in R^k$ ： 关系$r$ 的向量表示

SLM 的非线性操作仅提供了实体与关系之间比较微弱的联系，却引入了更高的计算复杂度。

### 3. 能量模型 - SME

SME 定义了两种损失函数：
$$
f_r(h,t) = (M_1l_h + M_2l_r + b_1)^T(M_3l_t  + M_4l_r + b_2) \\
f_r(h,t) = (M_1l_h \otimes M_2l_r + b_1)^T(M_3l_t \otimes M_4l_r + b_2)
$$

- $M_1, M_2, M_3, M_4 \in R^{d \times k}$ ：投影矩阵
- $\otimes$： 按位乘

### 4. 隐变量模型 - LFM

LFM 定义损失函数为：
$$
f_r(h,t) = l_h^TM_rl_t
$$

- $M_r \in R^{d \times d}$ ： 关系 $r$ 所对应的双线性变换矩阵。

LFM 通过基于关系 $r$ 的双线性变换，刻画了实体和关系的语义联系，协同性较好，计算复杂度低。

扩展： DISTMULT 模型

### 5. 张量神经网络模型 - NTN

基本思想： 用双线性张量取代传统神经网络中的线性变换层， 在不同维度下将头，尾实体向量联系起来。

损失函数定义如下：
$$
f_r(h,t) = u_r^Tg(l_hM_rl_t + M_{r,1}l_h + M_{r,2}l_t + b_r)
$$

- $u_r^T$： 一个与关系相关的线性层
- $g()$： tanh 函数
- $M_r \in R^{d \times d \times k}$， $M_{r,1}, M_{r,2} \in R^{d \times k }$

注意： NTN 中的实体向量是该实体中所有单词向量的平均值，这样可以充分利用单词向量构建实体表示，降低实体表示学习的稀疏性，增强不同实体的语义联系。

缺点是由于计算复杂度较高，因此需要更多的三元组数据才能充分学习，实验表明，NTN的确在大规模稀疏知识图谱上效果较差。

### 6.  矩阵分解模型 - RESACL 模型







### 7. 翻译模型 - TransE

TransE认为，对于三元组 $(h,r,t)$ ， 应该具有以下的关系：
$$
l_h + l_r \approx l_t
$$
TransE 模型定义的损失函数为：
$$
f_r(h,t) = |l_h + l_r - l_t|_{L_1/L_2}
$$
在实际训练中，为了增强知识表示的区分能力，TransE 采用最大间隔方法，定义了如下优化目标函数：
$$
L = \sum_{(h,r,t) \in S} \sum_{(h',r',t') \in S'} max(0, f_r(h,t) + \gamma - f_{r'}(h', t') )
$$

- S： 合法三元组的集合
- S‘： 错误三元组的集合，TransE 将 S 中每个三元组的头实体，关系，尾实体其中之一随机替换为其他实体或关系来得到 S'
- $\gamma$： 合法三元组得分与错误三元组得分之间的间隔距离

优点：参数少，计算复杂度低，却能直接建立实体和关系之间的复杂语义联系。

实验表明，TransE 的性能较以往模型有显著提升。特别是在大规模稀疏知识图谱上，TransE 的性能尤其惊人。

## 3. 知识表示学习的主要挑战

### 1. 复杂关系建模



### 2. 多源信息融合



### 3.  关系路径建模





## 3. 发展趋势

### 1. 符号 + 向量

一个新的趋势是把符号逻辑与表示学习结合起来研究更加鲁棒、易于捕获隐含知识、易于与深度学习集成、并适应大规模知识图谱应用的新型表示框架。这需要较好的平衡符号逻辑的表示能力和表示学习模型的复杂性。一方面要能处理结构多样性、捕获表达构件的语义和支持较为复杂的推理，另一方面又要求学习模型的复杂性较低。

