# ElMo

每一个词语的表征都素hi整个输入语句的函数，具体做法就是在大预料上以语言模型为目标训练出双向LSTM模型，然后利用LSTM产生词语的表征.ELMo因此得名。为了应用在下游NLP任务中，一般先利用下游任务的语料库(这里忽略掉label)进行语言模型的微调，这种微调相当于一种domain transfer(域转移)。然后才利用label的信息进行有监督学习。

ELMo表征是深的，就是硕他们是biLM的所有层的内部表征的函数，这样做的好处是能够产生丰富的词于表征。高层的LSTM的状态可以捕捉词于意义中和语境相关的哪方面的特征(比如可以用来做语义的消歧)，而底层的LSTM 可以找到语法方面的特征(比如可以做词性标注)。如果把它们结合在一起，在下游的NLP任务中会体现优势。





​                 

**细节一：elmo的网络结构是双向双层的lstm,如何实现双向的lstm的呢？**

**与Bert在预训练目标中使用masked language model来实现双向不同，ELMo的双向概念实际是在网络结构中体现的。**输入的embedding通过lstm的hidden state作为正向输出，embedding做reverse后的结果再通过lstm的hidden state反向输出，正向输出与反向输出做concat。最后输出实际是个lauguage model，基于前面的词计算下一个词的概率。

**细节二：与Bert的相比，ELMo微调是如何实现的？**

**ELMo的微调从严格意义上来说，不是真正的微调，预训练网络结果是fix的。** 整体来说，是把句子输入到预训练网络的embedding，与下游任务word embedding做concat，concat的结果整体作为下游NLP任务的输出。图为上游ELMO网络迁移到下游阅读理解bidaf网络中。

![img](https://pic3.zhimg.com/80/v2-b98c4b157e1f8829da7203da227320ba_1440w.jpg)ELMo迁移到下游NLP任务

**细节三:ELMo的语义表示输出是如何处理的？**

**ELMo**的语义表示输出计算公式如下：

![img](https://pic3.zhimg.com/80/v2-273444a6fbc4c5d7286153197bc0e33e_1440w.jpg)



其中，hk 代表三层的输出（分别是embedding层，第一层lstm，第二层lstm），与W相乘的权重参数， 通过 个softmax得到 , 权重参数W加入 L2 正则，防止拟合。

r是整体的缩放因 ，为的是与下游的任务 embedding概率分布统一 。

代码里还有个小技巧，word embeding层是256维，所以他把相同word embedding做个concat，与lstm输出的516维统一起来。

**从这些细节处理可以看出ELMo在微调阶段的处理与Bert有挺大的不同。**

**细节四：elmo适合哪些下游NLP 任务？**

**ELMo在短本任务上表现好。**ELMo迁移到下游网络中， 一个是答案较短的数据集，提升有3-4个点， 一个答案较长的数据集，提升只有0.5 左右。在实验中，我们对比过词法分析和阅读理解任务，其在词法分析效果好于阅读理解。

**细节五：elmo还有哪些值得注意的参数细节？**

**1.** dropout的处理方式：训练时候设置为0.5，为防止过拟合。infer阶段dropout设置为0。

2.r值对微调阶段影响很大。r值对语义表示的输出调节与下游NLP任务的阈值有较大帮助。

3.No L2 和 L2效果差不多。elmo的语义表示输出公式中权重参数W中加入了L2正则，从实验结果来，没有L2正则对结果影响不大，猜想可能是dropout的设置过大，导致L2对结果不产生影响。

![img](https://pic2.zhimg.com/80/v2-f5877764d401a70afbe596b6b89f7995_1440w.jpg)图为ELMo迁移到下游网络的结果，红色的baseline为百度词法分析LAC。

**细节六: 如何训练自己的elmo中文预训练模型？**

准备约3G的中文文档数据。GPU：8卡GPU，显存大于22GB，调节batch_size，适合于显存大小，最大限度利用显存资源。训练时间约为一周，7天。

**细节七：ELMO微调训练周期长的下游任务，如何在较短时间看是否靠谱？**

有个技巧是把finetune阶段dropout往大了调，使其快速过拟合。看峰值是否符合预期。