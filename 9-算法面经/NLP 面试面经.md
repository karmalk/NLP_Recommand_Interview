# NLP 面试面经

1. 基础的数据结构：插入排序、选择排序 （记下时间复杂度）， 链表新增、删除，二叉树的遍历，其他场景算法题大多出自leetcode
2. 逻辑回归损失是什么， 手动推导一遍
3. 对集成学习， SVM的理解，以公式的形式写出来最好
4. 对HMM ，CRF的理解， CRF的损失函数什么，维特比算法的过程
5. 手写一个tfids
6. word2vec的CBOW与SkipGram模型及两种训练方式(负采样\层级softmax)， 两种训练方式的区别和应用场景,
    7.word2vec和fasttext的区别， 训练word2vec的有哪些重要参数
7. LSTM的单元结构图和6个公式要记住
8. 有几种Attention, Attention和self-Attention是具体怎么实现的，对应什么场景
9. BERT的模型架构，多少层，什么任务适合bert，什么任务不适合，应用在你写的项目改怎么做
10. tensorflow手写一个卷积代码， BILSTM + CRF模型的原理，记住常用基础api(比如jieba添加默认词典api，分词api)







# 模型篇一些需要知道的

- 预训练语言模型：GPT GPT2.0 BERT ERNIE ERNIE MASS UNILM
- 词向量：Skip-Gram CBOW Cove ELMO
- 文本分类：TextCNN TextRCNN HAN DPCNN 
- SQUAD 阅读理解： BiDAF， QANet， R-net， AOA等
- RACE 阅读理解：DCMN  OCN
- 文本相似度匹配：
- query-response 匹配

