# 预训练语言模型发展及使用

---

[TOC]

## 1. 预训练语言模型诞生

### 1. AR 与 AE 语言模型

**AR：Autoregressive Language Modeling(自回归语言模型)**

**AE： Autoencoding Language Modeling(自编码语言模型)**

- AR 语言模型：指的是，依据前面（或后面）出现的 tokens 来预测当前时刻的 token， 代表有 **ELMO， GPT** 等
  $$
  forward: p(x) = \prod_{t=1}^T p(x_t | x_{<t}) \\
  backward: p(x) = \prod_{t=T}^1 p(x_t | x_{>t})
  $$

- AE 语言模型：通过**上下文信息**来预测被 mask 的 token， 代表有 BERT , Word2Vec(CBOW)
  $$
  p(x) = \prod_{x\in Mask} p(x|context)
  $$


二者有着它们各自的优缺点：

- AR 语言模型：

  > - **缺点：**它只能利用单向语义而不能同时利用上下文信息。 ELMO 通过双向都做AR 模型，然后进行拼接，但从结果来看，效果并不是太好。
  > - **优点：** 对生成模型友好，天然符合生成式任务的生成过程。这也是为什么 GPT 能够编故事的原因。

- AE 语言模型：

  > - **缺点：** 由于训练中采用了 [MASK] 标记，导致预训练与微调阶段不一致的问题。 此外对于生成式问题， AE 模型也显得捉襟见肘，这也是目前 BERT 为数不多实现大的突破的领域。
  > - **优点：** 能够很好的编码上下文语义信息， 在自然语言理解相关的下游任务上表现突出。

### 2.  Feature-base pre-training：Word Embedding 到 ELMO

考虑到词向量不能解决词的多义性问题，在 ELMO 之前，我们往往采用双向 LSTM 来减轻这种问题，但这毕竟治标不治本，对于大数据集好说， 深层双向 LSTM 的确能够很好的缓解这种问题，但对于小数据集，往往没啥效果。

为了解决这种多义性问题，ELMO 在训练语言模型时采用双向 LSTM 。 不同层的 LSTM 能够把握不同粒度和层级的信息，比如浅层的 LSTM 把握的是单词特征， 中层的 LSTM 把握 句法 特征， 深层的 LSTM 把握语义特征， 对于不同的任务来说， 不同的特征起到了不同的作用。 

举例来说： 文本分类问题为何 ELMO 与 BERT 所起到的作用与 Word2Vec 相差无几，这就是因为对于分类问题来说， n-gram 信息起到很大的作用，而这本质就是单词特征； 但对于阅读理解领域， ELMO 与 BERT 就能大幅提高模型效果，这也是因为 语法与语义特征对于阅读理解这种深层次问题是十分重要的。

ELMO 在迁移到下游任务时，会将不同层的特征采用**加权求和**的方式来获得每个词的最终表示。

事实证明， ELMO 的确解决了多义性问题， 词性也能对应起来了。

但， ELMO 的缺点也十分明显：

- **LSTM 特征抽取能力远弱于 Transformer ， 并行性差**
- **拼接方式双向融合特征融合能力偏弱**

### 3.  Fine-tuning pretraining：  GPT 的诞生

GPT 虽然不是第一个预训练语言模型，但它的出现更具**开创意义**。其特点很明显：

- 采用**单向 Transformer** 作为特征抽取器
- 采用二阶段： 预训练 + 微调  来适配下游任务

GPT 1.0 与 GPT 2.0 的出现说明了一下几点：

- 高质量，大规模的预训练数据集是提升性能的根本
- 深层的 Transformer 模型具有更强的表示能力

至少，从目前为止， 业界还没有探索到数据与模型的极限，即仅仅堆数据，加深模型这条路，还没有走完。

### 4. 预训练新时代：BERT

GPT 虽然很强，但由于其基于 AR 模型且目前很多排行榜都是基于**自然语言理解**的，因此， GPT 在这方面无法与 BERT 的表现相抗衡。但 GPT 在生成方面是 BERT 无法比拟的， 就问你BERT： 会编故事吗？

BERT 主要分为两大部分： **Masked LM** 与 **NSP** (Next Sentence Prediction)。

BERT 由于其采用 AE 模型，MASK 操作所带来的缺陷依旧存在：

- 预训练与微调阶段不匹配的问题，这点 BERT 提供了一个策略来减轻该问题
- Mask 掉的  token 之间关系被忽略的问题

此外，由于数据量，模型都十分大，如果每次只 mask 一个token，那么整个训练过程将变得极为漫长， 文章采用 mask 15% 的操作，是一个经验性的选择，是对模型训练效果与训练时长做出了一个权衡。

至于 NSP 任务，事实证明其在句子关系上的确起到了一定的作用，对于某些任务的确有帮助，但也有文章指出，其实用处不大，这点后面会详细讨论。

## 2. BERT 之后的改进方案

BERT 之后，有诸多改进方案，无论是对语言模型进行改进，融合知识图谱进行改进，多任务学习+预训练语言模型等， 这些文章都具有很大的价值，且质量都很高，本节的目的是对最近的这些模型进行一个全面的总结，帮助人们理清思路。

### 1. 预训练 + 知识图谱

预训练诞生之后， 在自然语言理解领域的确获得了很大的提升，尤其是在阅读理解领域，完全超过了人类的表现，虽然这并不表示真正的智能，但依旧意味着，NLP 已经逐渐走向成熟。

随之而来的问题十分明显， 如何表示知识， 有没有一种方式能够利用**大规模语料+预训练语言模型**使得模型能够学习到知识，从而应用到下游任务中。相信这个课题将是接下来一个十分核心的热点， 百度和清华就这方面做出了探讨， 具体可参加： [Bert 改进： 如何融入知识](https://zhuanlan.zhihu.com/p/69941989)

百度的文章中提出通过 mask 掉实体来获取实体的表示， 可以肯定的是，这样是能够更好的表示实体信息，但对于实体关系的把握，我个人觉得存疑，这是因为 mask 操作往往不仅仅 mask 掉一个实体，那么被 mask 掉的实体之间的关系如何把握？

我个人觉得可以设计一个**精巧的任务**来验证实体之间的关系， 可以通过知识图谱来生成一个语料， 如：

```
谢霆锋是张柏芝的__。 
```

我们来预测空白处的位置， 判断其是否为 `丈夫`， `前夫` 之类的词， 这点需要根据具体的知识图谱而定。

清华的那篇文章，其先编码实体与实体间关系信息为一个向量， 然后将向量融合如预训练语言模型中进行训练， 而实际的操作更为复杂，俺个人觉得，这条路恐怕不是正确的路，不符合大道至简的原则，且任务太多，反而会引入噪声（个人对知识图谱研究不深，只是直观感觉）。

目前来看，个人觉得百度的路是对的。

### 2. 预训练 +  自然语言生成

这部分包含两个课题： 

- **如何将 BERT 用于生成任务**
- **如何设计一个适合于生成任务的语言模型**

前面在 AR 与 AE 模型中已经介绍过为何 BERT 不适用于生成任务中， 那么随之而来的问题就是，既然预训练语言模型在自然语言理解中如此成功，那么我们怎么将其迁移到自然语言生成中呢， 这是一个很大的问题，个人觉得还需要1年以上的时间发展才能出现类似 Bert 这样的突破。

我个人前期看了两篇文章，大致提了一下思路：[Bert 之后：预训练语言模型与自然语言生成](https://zhuanlan.zhihu.com/p/70663422)

首先，对于第一个课题： **如何将 BERT 用于生成任务。** 从技术上来说， Encoder-Decoder 架构应该是首选的框架了， Encoder 输入原句子，Decoder 生成新句子，那么问题在于，Encoder 与 Decoder 如何表示？

对于 Encoder 端来说，我们只需要将 Bert 直接初始化就行；那么对于Decoder 端呢？ 也采用 Bert 初始化吗？ 要知道的是， Decoder 可是用来生成的， 如果你的 embedding 信息是通过 AE 模型训练得到的，那么生成效果估计会诡异的一批。 那么现在的问题就变成了， **如何合理的初始化 Decoder 端的 embedding 信息呢？**

然后，我们再来谈谈第二个课题：**如何设计一个适合于生成任务的语言模型。** 目前从我看到的两篇文章中有两个思路：

- MASS 通过 mask 连续的**一小段**来试图即学习到理解知识，又学习到生成知识， 通过预测一段连续的 tokens 的确有助于提高模型生成方面的能力，但我个人觉得 mask 一小段信息所提升的生成能力十分有限， 且我认为这会影响到模型理解方面的能力。
- UULM 就厉害了， 它涉及了一组语言模型： **Unidirectional LM， Masked Bidirectional LM， Seq2Seq LM**， 真的是有钱任性， 但这样直接堆语言模型的方式真的好吗？ 可以肯定的是， 不同语言模型的结合必然是接下来的一大趋势，但你这样直接堆是不是有点暴力啊，我个人感觉一般。

那么，怎么去设计一个适合于生成任务的语言模型呢？ 我个人的想法在之前的博客提到了： 就人类而言， **生成是基于理解的，而非独立的， 在大脑中， 理解与生成是两个区域， 先理解后生成，这才是正确的路。** 

因此，我个人觉得，接下来的一个思路应该是： **理解的归理解，不断提高预训练语言模型在理解领域的表现， 对于生成，采用 Encoder-Decoder 框架。** 在预训练的角度来说， 基于理解层面训练得到的模型， 然后分别初始化 Encoder-Decoder 端， 然后去预训练 Decoder 端的参数， Freeze/not Freeze Encoder 端的参数， 从而得到词在 Encoder 与 Decoder 的不同 Embedding， 然后再生成任务中的 Encoder-Decoder 中分别使用这两种 embedding。 

### 3.  预训练 + 多任务学习

多任务学习就更好玩了，目前主要有两大代表： MT-DNN 与 ERNIE 2.0。

- **MT-DNN** 又叫**联合训练**，其实就是将预训练语言模型用在多个任务中去接着预训练，从而提高模型泛化。具体来说，训练过程就是把所有数据合并在一起，每个batch只有单一任务的数据，同时会带有一个task-type的标志， 然后shuffle 之后进行训练。

- **ERNIE**  提出一个很好的思路： **Continual Learning**。 这点很有意思，就像人类做题一样， 它不像 MT-DNN 那样训练，而是这样：

  ```
  task1 --> task1,task2 --> task1, task2, task3
  ```

  即在训练后续任务时，前面的任务依旧要参与训练，主要是希望在学习后续任务时依旧记得前面任务的学习成果。

我个人觉得 ERNIE 更符合我们人类的训练方式，不过具体的两种学习方式的表现还需要对比一下。 

回想我们人类的学习方式，其最初是专题训练，即每个 task 分别训练， 然后再进行总体训练，即所有 task 一起进行训练，然后发现自己的弱点，然后适当加强对某任务的训练，然后又进行总体训练，如此反复， 过程更像是这样：

```
(task1 or task 2 or task3)--> (task1, task2), (task1, task3), (task2, task3) --> (task1, task2, task3) --> (task1 or task2 or task3) --> ...
专题训练 --> 组合训练 --> 总体训练 --> 专题训练 --> ...
```

如果要保证训练新任务时不会过分忘记前面训练所得到的成果，似乎各个任务的训练样本比例以及训练时间更加重要。比如你做了一年的阅读理解，突然让你做单向选择，你答的也不会太好。

因此，我个人觉得， **联合训练 + Continual Learning** 是一个不错的思路。

不过我很疑惑的是，为何 7月份 有段时间 ERNIE 2.0 很火，我感觉它的创新性和各方面也就是 MT-DNN 一级别的啊，难道是宣传问题？ 

### 4. 改进语言模型

要说起改进语言模型，当首推 **XLNet**， 毕竟前段时间也是刷了榜的，通过交换 token 位置来解决 mask 所带来的预训练与微调不匹配的问题， 这似乎比 BERT 更加优秀。

但从最近的实验看来，似乎又不是那么回事， XLNet 精巧的语言模型设计有没有超越 BERT， 目前学界还没有一个定论，RoBERTa 的出现似乎验证了在同等数据集下，XLNet 并不占优势， 通过精调模型参数，**RoBERTa** 获得了十分漂亮的结果。

而 XLNet 对此予以回击，又在同等条件下对比了 XLNet 与 BERT 模型， 又说明了 XLNet 效果的确要超过 BERT，emmm， 俺也不知道该相信哪个，反正我都会试试，哪个好用哪个。

XLNet 网上讲的很多了，我就不细说了。

### 5. 预训练 + 中文领域

十分推荐： [BERT-WWM](<https://github.com/ymcui/Chinese-BERT-wwm>)

对于中文领域，分词还是分字一直是一个问题，那么，到底是选分词，还是分字，这一直是一个大问题。 

BERT 无疑选择了分字这条路， ERNIE 通过融入知识，其实带来了部分分词的效果，那么在预训练语言模型中，分词到底有没有用， BERT-WWM 给出了答案。

通过采用 mask 词的方式， 在原有的 BERT-base 模型上接着进行训练， 这其实有种 词 + 字 级别组合的方式， 我在 [深度学习时代，分词真的有必要吗](<https://zhuanlan.zhihu.com/p/66155616>) 中就有提到 字级别 与 词级别之间的差别， 而预训练语言模型能很好的组织二者，的确是件大喜事。

而事实证明， BERT-WWM 在中文任务上的确有着优势所在，具体就不细说了，至少目前来说，我们的中文预训练语言模型有三大选择了： BERT , ERNIE, BERT-WWM。

### 6. 预训练 + 精细调参

通过精细调参， BERT 能够发挥出更大的威力。 RoBERTa 证明了这一点。

此外， RoBERTa 认为 NSP 不仅不能带来下游任务的性能提升，反而会有所损害。 RoBERTa 的出现说明 BERT 本身的还有很多潜力要挖。

总的来说，这篇文章依旧是个苦工活，虽创新度一般，但价值很高。

### 7. 预训练+ 基础单元

大多数语言模型都采用 Transformer 来作为预训练的基本单元，那么 Transformer 有没有改进的空间呢？ 必然是有的。

XLNet 采用 Transformerxl 作为基本单元来解决长文本问题，Transformerxl 本质上就是 Transformer + 循环机制， 这样会带来并行性上的损失。

相信后续还会有更多的变体来解决 Transformer 的各种问题， 如果有对 Transformer 研究十分深的同学欢迎补充一下。

# 预训练语言模型使用

------

## 3. 如何使用预训练语言模型

### 1. 预训练，训练，微调

- **预训练：**指的是大公司通过大规模数据，大型网络所训练得出的模型，模型参数量往往很大

- **训练：** 指的是在预训练语言模型的基础上，再添加一些语料，接着训练语言模型，这对硬件的要求也很高，一般实验室玩不起。

- **微调：**指的是，不针对语言模型，而是针对特定任务，对上层模型与预训练语言模型进行微调，其实本质上还是对上层模型进行微调，对预训练语言模型进行微调在数据量较小的情况下所起到的作用不大。

  一块16gb 以上的显卡，你玩起来才能不那么吃力， 1080ti 对短文本有效，对长文本，基本无法微调。

### 2.  微调方法

- 方案1： 不冻结预训练语言模型， 进行全局的微调。
- 方案2：冻结预训练语言模型， 对上层模型进行微调
- 方案3： 先用现有数据在现有预训练语言模型上接着训练语言模型， 然后再在上游任务上进行微调。

效果比较： 方案3 > 方案2 > 方案1。

迭代速度比较：方案1 > 方案2 > 方案3。

## Reference

[1]  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

[2]  ERNIE - Enhanced Language Representation with Informative Entities

[3]  ERNIE - Enhanced Representation through Knowledge Integration

[4]  ERNIE 2.0 - A Continual Pre-training Framework for Language Understanding

[5]  MASS - Masked Sequence to Sequence Pre-training for Language Generation

[6]  RoBERTa - A Robustly Optimized BERT Pretraining Approach

[7]  UNILM - Unified Language Model Pre-training for Natural Language Understanding and Generation

[8]  XLNet - Generalized Autoregressive Pretraining for Language Understanding







